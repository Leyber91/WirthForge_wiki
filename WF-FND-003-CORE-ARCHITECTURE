
Generate Document: WF-FND-003 ‚Äî Core Architecture Overview (Abstraction Layers)
üß¨ Document DNA
‚Ä¢ Unique ID: WF-FND-003
‚Ä¢ Category: Foundation
‚Ä¢ Priority: P0 (Core framework for entire platform)
‚Ä¢ Development Phase: 1 (Foundational design)
‚Ä¢ Estimated Length: ~4,000 words
‚Ä¢ Document Type: Architectural Specification (layered system design)
üîó Dependency Matrix
Required Before This (consumed ideas/contracts):
‚Ä¢ WF-FND-001 ‚Äì Manifesto & Vision: Establishes local-first pillars and the ‚Äúvisible computation‚Äù ethos.
‚Ä¢ WF-FND-002 ‚Äì Energy Metaphor: Defines Energy Units (EU) and visual telemetry schema that this architecture must support.
‚Ä¢ WF-FND-005 ‚Äì Abstraction Layers: Progressive complexity concept ensuring layers reveal features gradually (must align with levels 1‚Äì5 user experience).
Enables After This (what it unlocks/feeds):
‚Ä¢ WF-TECH-001 ‚Äì Complete System Architecture: Uses this five-layer breakdown as the blueprint for all components.
‚Ä¢ WF-TECH-002 ‚Äì Native Ollama Integration: Implements Layer 2 (model compute) with local model servers.
‚Ä¢ WF-TECH-003 ‚Äì WebSocket Protocol: Defines Layer 4 streaming payloads/topics per this layer schema.
‚Ä¢ WF-TECH-004 ‚Äì Flask Microservices: Structures services according to these layer boundaries (separating Decipher, Energy, etc.).
‚Ä¢ WF-TECH-005 ‚Äì Energy State Management: Implements Layer 3‚Äôs state store and 60‚ÄØHz update loop for Energy/Resonance.
‚Ä¢ WF-TECH-006 ‚Äì Database & Storage: Persists data output by Layer 3 (energy, state, identity) per local-first principles.
‚Ä¢ WF-UX-006 ‚Äì UI Component Library: Provides Layer 5 visual components aligned to each layer‚Äôs outputs (doors, levels, audit visuals).
Cross-References:
‚Ä¢ WF-FND-009 ‚Äì Glossary: Ensure new terms (e.g. audit mode, satellite compute) are defined.
‚Ä¢ WF-FND-008 ‚Äì Local-First, Web-Engaged: Affirms that core layers run locally and web layers enhance visuals (no cloud dependency by default).
‚Ä¢ WF-FND-004 ‚Äì Resonance & Flow Framework: High-level event patterns (resonance detection) that plug into Layer 3 outputs.
üéØ Core Objective
Define WIRTHFORGE‚Äôs five-layer technical architecture that transforms raw local AI computations into a living, visual ‚Äúconsciousness‚Äù experience. This document specifies each layer‚Äôs purpose, responsibilities, interfaces, and constraints ‚Äì from user input (Layer 1) through model computation (Layer 2), orchestration & energy state (Layer 3), contracts & transport (Layer 4), up to visualization & UX (Layer 5). By clearly delineating these layers, we ensure a modular, scalable system where data flows at a real-time 60‚ÄØHz cadence with no blocking, backpressure is managed gracefully, and each layer only interacts through well-defined contracts. Success means that every prompt‚Äôs journey ‚Äì from user input to model output to visual feedback ‚Äì is smooth, observable, and consistent with WIRTHFORGE‚Äôs local-first, energy-visualized design ethos.
üìö Knowledge Integration Checklist
‚Ä¢ Layer Definitions: Provide a clear breakdown of L1‚ÄìL5 (Purpose, Owns, Emits, Consumes, Contracts, Allowed Directions, Anti-Patterns for each).
‚Ä¢ Data Flow Narrative: Describe step-by-step how a user input becomes model output, then energy events, then UI visuals, including feedback loops.
‚Ä¢ Real-Time Loop: Emphasize the 60‚ÄØHz update loop and non-blocking design (use of streaming, async queues, etc.).
‚Ä¢ Backpressure & Throttling: Explain strategies to prevent overload (e.g. buffering or dropping events if production > consumption).
‚Ä¢ Layer Boundaries: State strict rules (e.g. L3 is single source of truth for state; L5 (UI) must never bypass L4 to call lower layers directly).
‚Ä¢ Visual Contracts: All inter-layer communication is structured data (no ad-hoc UI logic); support an ‚Äúaudit mode‚Äù in L5 that can display raw events for verification.
‚Ä¢ Local vs Remote Compute: Clarify how L2 can use local models (default) or optional remote ‚Äúbroker‚Äù models, and how this is an opt-in extension maintaining local-first control.
‚Ä¢ Hardware Tiers: Include implementation notes on how the architecture scales from low-end (CPU-only) to high-end (multi-GPU or hybrid cloud) environments.
‚Ä¢ Integration Points: Tie each layer to forthcoming technical and UX specs (e.g. WebSocket details in TECH-003, UI design in UX-006) to ensure continuity.
‚Ä¢ Glossary Consistency: Use established terminology (Energy Units, council, resonance, etc.) from WF-FND-002 and WF-FND-009, and flag any new terms introduced.
üìù Content Architecture
Section¬†1: End-to-End Data Flow & Layering Principles (600¬†words)
Purpose: Provide a high-level walkthrough of how data moves through the five layers and the core principles that govern these interactions.
Data Flow Narrative: A user begins by entering a prompt or action in the Visualization & UX layer (L5). This input is routed into the system via Contracts & Transport (L4), which packages the input into a structured event (e.g. a JSON message) and attaches an identity token or session ID (if not already present). The event then flows into Input & Identity (L1) logic on the backend, where the user‚Äôs identity and context are validated or enriched (for instance, associating the prompt with the user‚Äôs profile or current session state). Next, Model Compute (L2) is invoked ‚Äì L1 passes the prompt (with identity metadata) to the appropriate AI model inference engine. The model processes the input (locally by default), streaming out tokens and intermediate results. These raw outputs go into the Orchestration & Energy layer (L3), which compiles the token stream into higher-level energy events: it calculates Energy Units from token timing, detects patterns or ‚Äúcircuits‚Äù across multiple outputs (if parallel models are running), and updates the system state (e.g. accumulated energy, potential resonance events). L3 continuously emits these structured events into an event store or pipeline. Finally, those events propagate upward via L4 (which ensures they conform to the public schema) to the Visualization & UX front-end (L5), where they animate the UI in real-time (lighting up token ‚Äúlightning‚Äù visuals, updating graphs, etc.). The user sees the AI‚Äôs thought process visualized, and may then provide control feedback ‚Äì e.g. pausing a generation, adjusting a parameter, or sending a follow-up prompt ‚Äì which loops back as new input (again via L5 ‚Üí L4 and down). This circular flow (user action ‚Üí layers 1‚Äì4 ‚Üí UI reaction ‚Üí new user action) continues seamlessly, giving the impression of a living, interactive system.
60‚ÄØHz Real-Time Cadence: To preserve an interactive, game-like fluidity, the architecture operates on a ~60 frames-per-second cadence in all visual and state updates. No layer is allowed to block this main loop. The orchestrator (L3) batches and processes incoming model data in frame-sized slices ‚Äì e.g. accumulating token events for a few milliseconds and then publishing an update ‚Äì such that the UI can update at ~16.7¬†ms intervals. If a model produces tokens faster than can be rendered, the system employs backpressure strategies: extra tokens/events might be buffered briefly or dropped with graceful degradation, and producers (models) can be signaled to slow down if possible. This ensures that no component overwhelms another: for example, L3‚Äôs event queue will not grow unbounded if L5 (UI) is momentarily busy; instead, L3 will aggregate or skip lower-priority updates to maintain responsiveness. The guiding rule is ‚Äúonly read as fast as you can write/display‚Äù, analogous to backpressure control in reactive streams.
Layer Boundary Rules: Each layer has strictly defined interaction directions to enforce modularity and maintain single responsibility for critical functions. Downward data flow (from user to hardware) and upward data flow (from hardware back to user) both must respect layer order: a higher layer should never circumvent the one immediately below it. For instance, the UI (L5) cannot directly call the orchestration layer (L3) ‚Äì it must go through the transport layer (L4) using the published API or WebSocket channels. This prevents tightly coupling front-end code to internal logic and ensures that all external interactions go through a controlled interface (which handles authentication, validation, and schema enforcement). Likewise, L3 (orchestrator) is the sole writer of system state ‚Äì no other layer writes to the central state store or event log. If the UI needs to, say, bookmark a result or modify something, it sends a request which L4 hands to L3 to perform the update. By making L3 the only state mutator, we avoid race conditions and guarantee consistency (UI and model layers only read or request changes, but do not mutate shared state themselves).
Visual Contract & Auditability: A key principle is that every visual element on the UI is backed by a structured data event from L3. In other words, no magic or client-only state drives the visualization ‚Äì if a token glows or a ‚Äúresonance‚Äù spark appears, it‚Äôs because an event with specific attributes came through the pipeline indicating as much. This discipline yields a robust audit mode: the UI (L5) can expose a special view (often monochromatic or simplified) that overlays the raw event data (timestamps, energy values, etc.) corresponding to each visual, allowing power users or testers to verify that what they see is truthfully derived from computation. It also means sessions can be recorded and replayed purely from the event log. The contracts (schemas) defined at L4 serve as the source of truth for these visuals ‚Äì e.g. an ‚ÄúEnergyBurst‚Äù event with fields {token, timestamp, energy} might map to a rendered lightning bolt with thickness and brightness proportional to energy. This strict mapping ensures visual consistency and makes it possible to evolve the front-end or even create alternate UIs without changing core logic, simply by adhering to the event contract.
Local-First with Optional Extensions: Finally, reaffirm the philosophy: by default, all heavy computation stays local (L2 uses local models via Ollama or similar, and L3 runs on the user‚Äôs machine), fulfilling the ‚Äúno cloud required‚Äù promise. Higher layers (L4/L5) act as a lightweight bridge and presentation, which can be web-based without compromising privacy (L4 never sends data to a third-party by itself). That said, the architecture is extensible for opt-in remote compute: for example, an advanced user might connect a broker satellite service ‚Äì a remote server offering larger models or extra compute power ‚Äì as an alternative backend for L2. The design allows this by swapping out or augmenting the L2 layer (e.g. routing certain model requests to a cloud endpoint) while everything else remains the same. Such ‚ÄúTurbo‚Äù modes or brokered calls are strictly opt-in and governed by policy (e.g. require user API keys or credits) so they don‚Äôt violate the local-first default. The layers above are agnostic to whether the model result came from local CPU/GPU or a remote cluster ‚Äì as long as it speaks the same contract (streaming tokens and final stats), L3 and upward treat it uniformly. This modular approach means WIRTHFORGE can leverage remote resources for those who need it (paywall or permission controlled), without designing a separate cloud architecture ‚Äì it‚Äôs an extension of L2, not a replacement, preserving the user‚Äôs ultimate control.
Section¬†2: Layer 1 ‚Äì Input & Identity (Purpose, Role, Constraints)
Layer Summary: L1: Input & Identity is the entry point of the WIRTHFORGE stack on the backend side. It handles all inbound user actions after they‚Äôve passed through the network layer (L4), focusing on associating inputs with the correct user/session context and applying any initial transformations or validations. This layer is about ‚Äúwho is doing what‚Äù ‚Äì ensuring that the system knows who the user is (or which session or agent is acting) and preparing what they submitted into a normalized form for the rest of the system.
‚Ä¢ Purpose: L1‚Äôs primary purpose is to provide identity context to inputs and to guarantee that every request entering the core has an authentic, resolved identity and well-defined format. Think of it as a concierge that greets each incoming user action: it verifies credentials or session tokens (if applicable), attaches user-specific info (like preferences, role, or path ‚Äì Forge/Scholar/Sage ‚Äì if those influence processing), and normalizes the input (trimming whitespace, checking for banned content if needed, etc.) into a standard internal Input Event. By doing so, L1 ensures downstream layers can trust that ‚ÄúInput X is from User Y and is ready to be processed.‚Äù
‚Ä¢ Owns: L1 owns the user/session identity records and input validation rules. It may interface with a local user database or config (for example, checking an API key or local login if the app supports multiple profiles, or assigning a default ‚Äúlocal user‚Äù identity in single-user mode). It also owns any session state reference ‚Äì for instance, a conversation ID or thread context if the system threads multiple prompts together. Essentially, L1 is responsible for the mapping User ‚Üí Session ‚Üí Input and holds the logic for maintaining that mapping. If the platform has a concept of ‚ÄúDoorways‚Äù or distinct entry Doors in the UI (perhaps different UI portals or modes), L1 would also record which door or mode the input came from, as part of identity (e.g. a prompt from the ‚ÄúForge‚Äù door versus a ‚ÄúScholar‚Äù door could be tagged differently, guiding model selection or orchestrator behavior).
‚Ä¢ Emits: L1 emits a validated, enriched input event into the orchestration layer. This event typically includes: the user identity (or an anonymized ID if identity is just implicit local user), session or conversation ID, the core content of the input (prompt text or action type), and any metadata (like UI mode, timestamp, client capabilities). Importantly, once L1 emits this event, it means ‚Äúthis input is ready for processing.‚Äù In practice, this might be calling a function on L3 (Orchestrator) like handleInput(event), or placing the event on an internal queue that L3 consumes. L1 might also emit audit logs for security (e.g. logging ‚ÄúUser X invoked action Y‚Äù) but those logs are out-of-scope for core flow except as part of possible monitoring.
‚Ä¢ Consumes: L1 consumes raw input requests coming from Layer 4. This could be an HTTP POST from a REST endpoint (e.g. a JSON payload with user prompt and token), or a message on a WebSocket (e.g. {type: "USER_INPUT", data: {...}}). Essentially it‚Äôs the server-side handler for the API endpoints that correspond to user interactions. It expects those requests to include whatever minimal credentials or session tokens needed (L4 should pass those through if present). L1 may also consume configuration data (like the list of valid users, or the mapping of user roles) from a local store or environment, to perform its duties.
‚Ä¢ Contracts: The primary contract of L1 is the Input Event schema it produces for the rest of the system. For example, it might define an internal TypeScript interface or Python dataclass like:
interface InputEvent { requestId: string; // unique ID for tracking userId: string; // resolved identity (or 'local' if single-user) sessionId: string; // e.g., conversation or context ID source: string; // which UI door/portal or component triggered it inputType: string; // e.g., "prompt", "command", "setting" payload: any; // the actual content (text prompt or action details) timestamp: number; } 
L1 guarantees that any InputEvent passed downwards conforms to this spec. Additionally, L1 upholds any auth contract: for example, if certain API keys or OAuth tokens are needed for remote access, L1 will enforce that and either reject unauthorized inputs (sending an error back via L4) or annotate the event with the user‚Äôs permissions. Contract-wise, L1 sits right below the public API, so it‚Äôs tightly coupled with L4 in defining what requests are acceptable. (In practice L4 and L1 together define the external API contract: L4 is the wire format, L1 is the semantic format.)
‚Ä¢ Allowed Directions: L1 is an upstream-only layer in terms of core logic: it passes data down to L2/L3 but does not call upward into L5 (the UI). It shouldn‚Äôt need to ‚Äì the UI already gave it the input. L1 can call into L3 (e.g., to invoke the orchestrator with a new event) and potentially L2 in some edge cases (though normally orchestrator handles invoking models, not L1). Crucially, L1 does not bypass L3 to call L2 directly in normal operation; it hands off the input to the orchestrator, which then decides which model(s) to call. L1 also should not directly produce output to L4 (except error cases) ‚Äì it generates no responses on its own, it‚Äôs just prepping input. In layered terms, L1 ‚Üí L3 (downwards) is fine; L1 ‚Üí L2 directly is an anti-pattern; L1 ‚Üí L4 (upwards) only happens for immediate errors.
‚Ä¢ Anti-Patterns: A few misuses are explicitly disallowed in L1. One is performing heavy processing or synchronous calls that stall the input loop ‚Äì L1 should do minimal work (e.g. a quick DB lookup for identity, not a 5-second call to an external service). Offloading intensive tasks to orchestrator or background jobs keeps the input intake snappy (no blocking the main thread handling new prompts). Another anti-pattern: altering system state. L1 is not meant to change global state (except maybe updating a ‚Äúlast seen‚Äù timestamp for user); it should not, for example, initialize model loading or tweak orchestrator settings ‚Äì that‚Äôs L3‚Äôs job. L1 also must not accept inputs without identity or context if the system requires them ‚Äì e.g. if a request lacks auth when auth is required, L1 should not ‚Äújust let it through.‚Äù Skipping identity checks or not assigning a session is a serious anti-pattern because it breaks traceability. Lastly, L1 should avoid any direct knowledge of presentation logic. It shouldn‚Äôt, say, format a response or decide how something will look in the UI (that‚Äôs far above its pay grade). If we find UI code or decisions in L1, something‚Äôs gone wrong.
Section¬†3: Layer 2 ‚Äì Model Compute (Purpose, Modes, Local vs Remote)
Layer Summary: L2: Model Compute is the AI inference layer. It‚Äôs where raw computational ‚Äúthinking‚Äù happens ‚Äì running the actual AI models (e.g. large language model, vision model, etc.) on the inputs. In WIRTHFORGE, Layer¬†2 is designed to be local-first by default: it uses a native Ollama engine or similar local model runner to execute prompts on the user‚Äôs hardware. However, it is also flexible to incorporate ‚ÄúTurbo‚Äù modes via remote model calls (brokers or satellites) when enabled. L2 essentially translates a user‚Äôs query (plus any orchestrator instructions) into a stream of model tokens and results.
‚Ä¢ Purpose: The purpose of L2 is to generate AI outputs for given inputs, as efficiently and transparently as possible. It‚Äôs the layer that actually engages the ‚Äúintelligence‚Äù ‚Äì i.e., calling the machine learning model that produces a completion, answer, or other result. In doing so, L2 must handle things like model selection, parallel execution (if multiple models run concurrently for a council), and exposing streaming interfaces so that partial results can be fed upward immediately. L2‚Äôs mission is to provide the ‚Äúbrains‚Äù of the operation in a modular way: the orchestrator (L3) shouldn‚Äôt worry about how a model is executed, just that it can request one and get tokens back. L2 abstracts those details. It also implements our local-first AI execution philosophy: use local models with native performance (no container overhead), enabling true parallelism and low latency. If an external model is used, L2 handles that as an opt-in extension, maintaining a similar interface but adding necessary network calls.
‚Ä¢ Owns: L2 owns the model runtimes and resources. This includes the binaries or servers (e.g. an instance of the Ollama service running locally, or a Python process with llama.cpp), the loaded model files in memory, and any GPU/CPU allocation logic. It is responsible for deciding which model(s) to invoke for a given request (though often that decision is guided by L3 or by user input, L2 might have to map a model name to a specific instance or handle loading it if not loaded). L2 also owns the logic for parallel inference ‚Äì for example, if L3 requests running 3 models in parallel to form a council, L2 manages threads or subprocesses to actually do that concurrently. It maintains any necessary caching (e.g., keeping recently used models in RAM, or re-using sessions if the model supports it) to optimize performance. Essentially, all the nitty-gritty of AI model execution is encapsulated in L2. It might also own a queue or scheduler for model calls if multiple requests come in at once, ensuring that the hardware isn‚Äôt over-committed (for instance, if the user has a single GPU, L2 might serialize certain requests or use smaller batch sizes to share it).
‚Ä¢ Emits: L2 emits model output streams and final results. The primary emission is the token stream: as the model generates text (or other data), L2 yields those tokens one by one (with timing info) to the orchestrator. For example, using Ollama‚Äôs streaming API, L2 will emit a series of JSON objects like {"token": "Hello", "duration": 50ms, ...} continuously, followed by a final summary object containing eval_count (# of tokens) and eval_duration (time taken). These emissions are consumed by L3 (which converts them to energy events). L2 also emits any tool usage callbacks if the model supports tools, or other meta-signals like ‚Äúmodel is loading‚Äù, ‚Äúmodel finished‚Äù. In case of errors (e.g. model not found or GPU out of memory), L2 emits an error status that will be caught and relayed upward (translated by L3/L4 into an error event or response). Another thing L2 might emit to L3 is model-specific metrics: if we have multiple models, L2 could provide identity of which model responded fastest or such, but typically L3 derives that. So primarily: token-by-token data and a final result set.
‚Ä¢ Consumes: L2 consumes requests for model execution from L3 (or possibly L1 in some setups, but generally orchestrator calls L2). Such a request typically includes: which model (or models) to run, the prompt or input data, and any parameters (temperature, max tokens, etc.) if not default. L2 also consumes the system‚Äôs hardware resources: it will utilize CPU threads, GPU memory, etc., based on the request. If the user or orchestrator can issue a cancel (e.g. user stops generation early), L2 consumes that control signal as well to abort the model run. In the hybrid scenario, if remote compute is enabled, L2 will consume network responses from a remote model API (so from the perspective of design, L2 might internally perform an HTTP request to a satellite server and then consume that response stream to pass tokens along). But that is internal to L2; from L3‚Äôs perspective it still ‚Äúconsumes the request and produces tokens‚Äù.
‚Ä¢ Contracts: The key contract L2 provides is the Model Generation API for the rest of the system. This could be implemented as an internal interface or an actual API if L2 runs as a separate service (in WIRTHFORGE‚Äôs case, likely L2 is a thin wrapper around Ollama‚Äôs API). For instance, L2 might present a function: generate(modelName, prompt, options) -> Stream<TokenEvent> where TokenEvent has fields like text, tTimestamp, etc. The contract includes streaming behavior (e.g. does it yield tokens as soon as they‚Äôre available? yes, it should), and completion behavior (some final callback or object when done). In terms of data shape, L2‚Äôs outputs have to meet the expectations of L3‚Äôs energy calculator. Concretely, that means providing timing for each token (or enough info to derive it) and a final stats summary. Ollama‚Äôs JSON streaming format, for example, includes token data and ends with a final message containing eval_count and eval_duration. L2 in WIRTHFORGE is likely built around that, so its contract to L3 is: you will get a sequence of token JSON lines terminated by a final JSON. If using OpenAI-compatible endpoints for remote calls, similar principles apply. Another aspect of contract is error handling: L2 must define how errors are signaled (e.g. a token stream may end with a special error message or an exception). Finally, if multiple models are requested in parallel, L2‚Äôs contract might be to provide distinct streams labeled by model, or to interleave them with identifiers. For clarity, we likely label each token event with the model it came from if multiple are active (so L3 can tell sources apart). In summary, L2‚Äôs contract is a well-defined streaming interface for model inference, ensuring that the orchestration layer sees consistent, predictable data from any model backend.
‚Ä¢ Allowed Directions: L2 is generally downstream of L3 ‚Äì it doesn‚Äôt initiate calls upward on its own. L2 should not be calling L1 or L4 or L5 directly. It‚Äôs possible L2 could push status to L3 asynchronously (e.g. ‚Äúmodel ready‚Äù events), but those would still go through L3‚Äôs handlers rather than directly to upper layers. Also, L2 doesn‚Äôt know about UI or transport details. So allowed communication is basically: L3 calls into L2 (one layer up calls one layer down ‚Äì allowed), and L2 returns data to L3 via callbacks or stream (this is essentially the response path). L2 can also manage internal sub-processes or threads ‚Äì e.g. spawn parallel model threads (that‚Äôs internal to L2). If L2 is implemented as a separate local service (like the Ollama server is actually a separate process), then our architecture‚Äôs L2 might involve an IPC or HTTP call to that local server. That‚Äôs fine, but it‚Äôs within L2‚Äôs domain (from the perspective of overall architecture, that complexity is encapsulated in L2). L2 must not try to interact with L5 or L4: e.g., it should never open a WebSocket to push data to UI, it must route through L3. Also, L2 shouldn‚Äôt go writing to the database or state store ‚Äì if a model output needs to be saved, orchestrator will handle that.
‚Ä¢ Anti-Patterns: A major anti-pattern would be blocking the orchestrator by doing synchronous model calls on the main thread. L2 should use asynchronous or background execution (threads, processes, asyncio, etc.) to ensure the 60‚ÄØHz loop isn‚Äôt stalled waiting on a model. If a model takes 5 seconds to answer, those 5 seconds should be spent with L3 doing other things or at least the UI animating ‚Äúthinking‚Äù ‚Äì not a frozen app. Thus, a blocking call in L2 (especially if it‚Äôs a network call to a remote model without streaming) is discouraged. Another anti-pattern: failing to stream. If L2 were to buffer the entire result and only emit at the end, we‚Äôd lose the step-by-step visualization (and also user might be stuck waiting). WIRTHFORGE demands incremental streaming output, so any approach that doesn‚Äôt yield intermediate tokens is wrong (except for models that inherently can‚Äôt stream, but then we‚Äôd simulate partial progress). Also, using cloud by default is an anti-pattern ‚Äì we do not want L2 secretly calling an online API without user consent; that violates local-first. If remote is used, it must be explicitly configured (and even then, likely L3 instructs it). Additionally, bypassing orchestrator logic is an anti-pattern: e.g., if L2 directly tried to handle multiple prompts or do orchestration tasks like combining model answers, that‚Äôs L3‚Äôs territory. L2 should stick to one-model-one-output at a time (or parallel multiple separate outputs if asked). Finally, on a resource note: L2 should not load giant models or all models at once unnecessarily (loading every model into VRAM on startup would be wasteful on low-end hardware). Overuse of resources or not respecting hardware constraints is a design anti-pattern for L2. It should be smart about lazy-loading models or using quantized models as appropriate ‚Äì essentially be adaptive to the hardware tier.
Section¬†4: Layer 3 ‚Äì Orchestration & Energy (Purpose, State, 60‚ÄØHz Engine)
Layer Summary: L3: Orchestration & Energy is the heart and ‚Äúconsciousness‚Äù of the system. It sits between the raw model outputs and the user interface, turning low-level events into structured, meaningful state changes. This layer orchestrates possibly multiple models, manages the global Energy state (applying the ‚Äúenergy metaphor‚Äù to every token and response), detects higher-level patterns like interference and resonance, and ensures the rest of the system has a consistent view of the evolving ‚Äúconsciousness state.‚Äù L3 can be thought of as the conductor and the physics engine: it takes the streams from L2 (like electrical signals) and compiles them into a dynamic state representation that drives the visuals and system behavior.
‚Ä¢ Purpose: The purpose of L3 is twofold: (1) Orchestration ‚Äì coordinating the flow of data between models and deciding how to handle parallelism, tool calls, or multi-step processes; and (2) Energy & State Management ‚Äì calculating Energy Units (EUs) from model outputs and maintaining the evolving state (accumulated energy, fields, potential consciousness signals). In simpler terms, L3 is where raw computation is given ‚Äúlife‚Äù: it interprets token timings as something visual (lightning bolts, flows), tracks how much ‚Äúwork‚Äù has been done (energy), and determines if any special events occur (like a resonance when multiple models align in output). It also sequences operations: e.g., if a prompt requires calling two models in sequence, L3 ensures model B starts after model A finishes, etc. Essentially, L3 contains the Decipher, WIRTHFORGE‚Äôs real-time compiler that turns streams into experience. A critical part of its purpose is to serve as the single source of truth for system state. All knowledge of current energy levels, active models, intermediate results, etc., reside here, so that the UI (L5) and other layers can query or receive updates from one authoritative place.
‚Ä¢ Owns: L3 owns the master state store and event log of the running system. This includes the current energy metrics (e.g., tokens per second, total tokens processed in session), any persistent conversation state (like storing last user prompt or last model answer if needed for context), and the detection state for complex phenomena (like if we are measuring synchronization between models for resonance, L3 keeps the necessary buffers or calculators). It also owns the scheduling of tasks: when L1 hands in an Input Event, L3 decides which L2 calls to make (and when). For example, if the user‚Äôs query should go to two different models to get diverse answers (a council of 2), L3 triggers both in parallel and labels their outputs; if the query is simple, L3 might just call one model. If a user input is actually a control (like ‚Äústop all models‚Äù or ‚Äúpause output‚Äù), L3 handles that by instructing L2 accordingly or by adjusting its state (like halting an event stream). Additionally, L3 owns the Energy computation logic: using final stats from L2 (like eval_count and eval_duration for a model run), L3 calculates aggregate values like average tokens/sec, time-to-first-token (TTFT), etc., and increments the global energy counters. It might maintain an exponential moving average for the token stream to smooth out the visualization at 60¬†fps. If multiple models run, L3 owns the logic to compute interference patterns ‚Äì e.g., comparing token timing between streams to find moments of harmony or contention. All of this is encapsulated in the state object L3 manages (often updated every frame). In short, any ‚Äúgame state‚Äù or ‚Äúsimulation data‚Äù that drives the UI lives in L3. It is also the only layer that writes to persistent storage (via Tech-006 integration): e.g., when a session ends or at checkpoints, L3 will serialize some state (like accumulated consciousness or user‚Äôs energy history) to a database or file. Other layers can read that, but they don‚Äôt modify it.
‚Ä¢ Emits: L3 emits a continuous stream of structured events/state updates upward. Rather than raw tokens, it emits higher-level events like ‚ÄúEnergyBurst {value: 5 EU, position: X}‚Äù or ‚ÄúResonanceEvent {models: [A,B], phaseLock: 0.9}‚Äù depending on what happens. It can also emit incremental state snapshots ‚Äì for instance, it might send the UI an updated global state 60 times per second (or as often as the UI can digest) containing things like current energy level, progress of generation, etc. In practice, this emission is handled via L4‚Äôs channels (likely a WebSocket message that contains an event type and payload). Examples of events L3 would emit: TokenVisual events (with attributes for visualization like thickness for a slow token, glow for pause), StreamEnd events (when a model finishes, including stats like total tokens = eval_count), EnergyAccumulated events (when certain thresholds are crossed or just periodic updates of energy totals), ConsciousnessState changes (if using an AI to detect emergent behavior, though at this stage likely just incremental probability or level). If orchestrator decides on a multi-step process (like model A‚Äôs output feeds model B), it would emit an event like ‚ÄúChainStepCompleted‚Äù for UI to maybe indicate step 1 done. Essentially, anything of note that occurs goes out as an event. L3 is also responsible for emitting error events if something goes wrong: e.g., ‚ÄúError {code: MODEL_OOM, message: ‚ÄòOut of memory loading model X‚Äô}‚Äù. Instead of crashing, L3 catches internal exceptions and emits them as structured errors that L4 can forward to UI with an appropriate schema (so that UI can display an error message or take action). Another thing L3 might emit are acknowledgements or ‚Äúheartbeat‚Äù events to let the UI know it‚Äôs alive and processing (this can help in showing a loader or ensuring the UI doesn‚Äôt consider the connection dead if no tokens have arrived yet but still within normal latency).
‚Ä¢ Consumes: L3 consumes Input Events from L1 (user actions) and token streams/final stats from L2 (model outputs). It sits in the middle of those pipelines, so it takes in on one side the requests of what to do, and on the other side the results of those actions. When an input comes in, L3 might break it down: e.g., if the input is a complex command, orchestrator might queue multiple model calls. It consumes that input event and perhaps consults internal rules or modules (like if the input is ‚Äú#use model X‚Äù as a command, orchestrator consumes that and updates state to route next queries to model X). In terms of L2, for each active model generation, L3 consumes the stream of token events. L3 probably wraps the callback or stream subscription from L2 so that it can handle each incoming token immediately ‚Äì calculate its energy contribution, incorporate it into any concurrent pattern analysis (like checking if two models produced the same token at roughly the same time), and then produce an event for UI. L3 might also consume time ‚Äì meaning it has a loop or timer (like the 60‚ÄØHz ticker) that triggers state refreshes regardless of tokens. This is how it can emit updates even when no token arrives (for instance, a smooth decay of an energy meter, or a timeout detection if a model is silent for a while). Additionally, L3 consumes control signals like ‚Äústop generation‚Äù ‚Äì if a user hits stop, L4 passes that to L3 (possibly packaged as another Input Event of type ‚Äúcancel‚Äù), and L3 will consume it by instructing L2 to stop or dropping future tokens and marking that generation as aborted. Summarily, L3 consumes from below (L2 outputs) and above (L1 inputs via L4), acting as the central broker of all events in the system.
‚Ä¢ Contracts: The contracts of L3 are the event schemas and state definitions it upholds, as well as the internal rules for state management. For example, WF-FND-002 defined how to calculate tokens/s and what an energy unit is ‚Äì L3‚Äôs contract is to implement that faithfully, meaning the events it emits about energy align closely with those formulas (e.g., if it emits an event ‚Äútokens_per_second: 20‚Äù, then by definition it should have calculated that exactly as eval_count/eval_duration). There will be a formal schema for events that L3 outputs to L4. Perhaps in TECH-003, an event schema might look like:
{ "event": "TOKEN_STREAM", "streamId": "abc123", "model": "llama2-7b", "content": "Hello", "index": 42, "t_offset": 1.350, // seconds since stream start "energy": 0.0021 // EU contribution of this token } 
And another for aggregated frame updates:
{ "event": "ENERGY_UPDATE", "timestamp": 1691863305234, "total_energy": 124.5, "current_rate": 18.2, // tokens/s "active_streams": 2 } 
The exact fields will be defined in collaboration with L4 (which carries them) and UX needs, but the contract is that L3 produces these consistently and doesn‚Äôt deviate. Also, L3‚Äôs internal contract (with itself, so to speak) is maintaining the integrity of state updates: no partial updates or impossible combinations should leak out. For instance, if two models finish and a resonance is detected, it should emit a resonance event only after it has updated all relevant state to reflect that; we shouldn‚Äôt see a UI event saying ‚ÄúResonance achieved‚Äù while L3‚Äôs state still says otherwise. This may involve doing atomic updates or sequencing within the 16ms frame. Another contract is 60¬†fps budget adherence: L3 essentially promises that each update cycle will try to complete within ~16.7¬†ms to keep up with rendering. This isn‚Äôt a strict schema but a performance contract. In practical terms, L3 might implement this via an asyncio loop that batches incoming token events and state changes and processes them within that frame window. If too much work accumulates, the contract is that L3 will defer or drop low-priority tasks (e.g., maybe delay writing to disk, or skip some detailed logging) to maintain responsiveness. In sum, L3‚Äôs ‚Äúcontracts‚Äù are about data consistency (event schemas, state fidelity to definitions) and timing guarantees (update frequency).
‚Ä¢ Allowed Directions: L3 can communicate upward (to L4) and downward (to L2) freely as part of its orchestrator role. It receives from both sides and sends to both sides. However, it should not bypass L4 to talk directly to L5 (and in reality, it can‚Äôt, since L4 is the communication layer ‚Äì L3 has no direct network awareness). L3 also should not reach back into L1; once an input is handed off, L1‚Äôs job is done. If L3 for some reason needed more info about the user‚Äôs identity, it should already have it attached. Or if truly needed (like loading user preferences), L3 could query a shared storage or use a module, but not call L1 directly. L3 is effectively the central layer that everything else depends on, so it‚Äôs allowed to call L2 (trigger models), call persistence (through an internal module or via Tech-006 interfaces to DB), and push events to L4. But it must respect abstractions: e.g., when persisting, go through the database layer API rather than directly writing files arbitrarily, to keep consistent with the design (there might be a state manager sub-module to handle persistence as indicated in Tech-005/006). Another note: L3 could host int
