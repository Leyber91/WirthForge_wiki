sequenceDiagram
    participant User as ğŸ‘¤ User
    participant UI as ğŸ¨ L5: UI/UX
    participant Transport as ğŸŒ L4: Transport
    participant Identity as ğŸ” L1: Identity
    participant Orchestrator as ğŸ§  L3: Orchestrator
    participant Model as âš¡ L2: Model
    
    Note over User,Model: User Prompt Submission & Token Streaming
    
    User->>UI: Enter prompt "Hello AI"
    UI->>Transport: WebSocket message {type:"INPUT", text:"Hello AI"}
    Transport->>Identity: Validate & attach user context
    Identity->>Orchestrator: InputEvent(userId, sessionId, "Hello AI")
    
    Note over Orchestrator: Orchestrator decides model strategy
    Orchestrator->>Model: generate(model:"llama2-7b", prompt:"Hello AI")
    
    Note over Model,Orchestrator: Real-time Token Streaming (60Hz)
    
    loop Token Generation Stream
        Model-->>Orchestrator: Token: "Hello"
        Orchestrator->>Orchestrator: Calculate energy: EU += f(token, timing)
        Orchestrator-->>Transport: Event{type:"TOKEN", token:"Hello", energy:0.021}
        Transport-->>UI: WebSocket push
        UI->>UI: Render lightning bolt (thickness âˆ energy)
        
        Model-->>Orchestrator: Token: " there"
        Orchestrator->>Orchestrator: Update state (60Hz frame)
        Orchestrator-->>Transport: Event{type:"TOKEN", token:" there", energy:0.019}
        Transport-->>UI: WebSocket push
        UI->>UI: Animate token flow
        
        Note over UI: Frame budget: 16.67ms max per update
        
        Model-->>Orchestrator: Token: "!"
        Orchestrator->>Orchestrator: Accumulate metrics
        Orchestrator-->>Transport: Event{type:"TOKEN", token:"!", energy:0.023}
        Transport-->>UI: WebSocket push
        UI->>UI: Update energy visualization
    end
    
    Model-->>Orchestrator: Final stats: {eval_count:3, eval_duration:150ms}
    Orchestrator->>Orchestrator: Calculate TPS: 3/0.15 = 20 tokens/sec
    Orchestrator-->>Transport: Event{type:"COMPLETE", tps:20, total_energy:0.063}
    Transport-->>UI: WebSocket push
    UI->>UI: Display completion metrics
    
    Note over User,Model: User Control Actions
    
    User->>UI: Click "Stop Generation"
    UI->>Transport: WebSocket {type:"CONTROL", action:"STOP"}
    Transport->>Orchestrator: Control signal
    Orchestrator->>Model: Cancel generation
    Model-->>Orchestrator: Ack: Generation stopped
    Orchestrator-->>Transport: Event{type:"STOPPED"}
    Transport-->>UI: WebSocket push
    UI->>UI: Update UI state to stopped
    
    Note over User,Model: Parallel Model Council (Level 2+)
    
    User->>UI: Enable "Council Mode" 
    UI->>Transport: WebSocket {type:"CONFIG", council:true}
    Transport->>Orchestrator: Update orchestration strategy
    
    par Model A Stream
        Orchestrator->>Model: generate(model:"llama2-7b", prompt:"Compare options")
        Model-->>Orchestrator: Token stream A
    and Model B Stream  
        Orchestrator->>Model: generate(model:"codellama-13b", prompt:"Compare options")
        Model-->>Orchestrator: Token stream B
    end
    
    Note over Orchestrator: Interference Detection
    Orchestrator->>Orchestrator: Detect sync: models output same token simultaneously
    Orchestrator-->>Transport: Event{type:"INTERFERENCE", pattern:"constructive"}
    Transport-->>UI: WebSocket push
    UI->>UI: Render interference visualization
    
    Note over User,Model: Backpressure Management
    
    Model-->>Orchestrator: High-speed token burst (>60 tokens/sec)
    Note over Orchestrator: Frame budget exceeded
    Orchestrator->>Orchestrator: Buffer tokens, batch for next frame
    Orchestrator-->>Transport: Batched events (max 3 per 16.67ms frame)
    Transport-->>UI: Throttled WebSocket messages
    UI->>UI: Smooth animation despite burst
    
    Note over User,Model: Error Handling
    
    Model-->>Orchestrator: Error: "GPU out of memory"
    Orchestrator->>Orchestrator: Log error, attempt recovery
    Orchestrator-->>Transport: Event{type:"ERROR", code:"MODEL_OOM", message:"..."}
    Transport-->>UI: WebSocket error event
    UI->>UI: Display user-friendly error message
