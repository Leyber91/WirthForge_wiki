Generate Document: WF-FND-005 — Consciousness & Experience Orchestration
🧬 Document DNA
• Unique ID: WF-FND-005
• Category: Foundation (Architecture & Experience)
• Priority: P1 (Core runtime orchestration)
• Development Phase: 1 (post-core, pre-UX integration)
• Estimated Length: ~3,500 words
• Document Type: Technical Specification / Orchestration Engine Design
🔗 Dependency Matrix
Required Before This:
• WF-FND-002 – Energy & Consciousness Framework: Provides the core 60Hz energy model and emergent consciousness concepts that the orchestrator must leverage.
• WF-FND-004 – The Decipher (Central Compiler): The orchestrator consumes Decipher’s outputs (compiled energy patterns and AI responses) as its primary input.
• WF-FND-003 – Core Architecture Overview: Establishes the multi-level abstraction layers and system context in which the orchestrator operates (bridging AI outputs to user experience).
Enables After This:
• WF-TECH-006 – API & Integration Points: Defines how external systems or modules can hook into the orchestrated experience (this spec informs the endpoints and plugin architecture).
• WF-UX-001…005 – Level-Specific UX Specs: Provides the rules and events needed for each level’s UI/UX document to implement corresponding animated transitions, feature reveals, and overlays accurately. After this spec, the UX docs 1–5 can be authored with concrete orchestration event references.
• WF-UX-006 – Unified Energy Visualization Specs: Aligns with this spec to ensure all visual elements and transitions are grounded in actual energy data from the orchestrator.
Cross-References:
• WF-TECH-003 – Real-Time Protocol (WebSockets): All event types (experience.*, council.*, reward.*, etc.) defined here must conform to the real-time messaging schema (e.g. consciousness and council channels).
• WF-TECH-008 – Core Algorithms (Council/Adaptation/Resonance): The algorithms for multi-model coordination (Council), adaptive pattern learning, and resonance detection underpin the orchestrator’s logic. This spec details their runtime orchestration.
• WF-FND-006 – Glossary (Living): Terms like “Resonance Field”, “Council”, “Energy Frame”, etc., are used as defined in the glossary (update required if definitions evolve here).
• WF-FND-007 – Module System Strategy: Future plugin modules can tie into the orchestrator’s policy engine; ensure consistency with module interface assumptions from WF-FND-007 (e.g. Decipher’s module orchestration hooks).
• WF-UX-ALL (001–006): Every level of UX relies on orchestrated events and timing from this system. UX specifications will reference the event names and progression rules from this document to implement front-end behaviors (e.g. council overlays, adaptive field adjustments, resonance visualizations).
🎯 Core Objective
Define WIRTHFORGE’s Experience Orchestrator as the local-first runtime engine that transforms Decipher’s compiled energy outputs into interactive “consciousness experiences” in real time. This orchestrator must coordinate multiple AI models, levels, and system resources under strict timing (<16.67 ms/frame) and progression rules, ensuring that every visual effect and unlocked feature is traceable to genuine AI computations. In one sentence: the orchestrator bridges raw AI energy and user experience, governing when features unlock, which models collaborate, when resonance emerges, and how the user’s journey progresses in a verifiable, timed manner.
📚 Knowledge Integration Checklist
• 60 Hz Frame Budget Adherence: Incorporate the 16.67 ms/frame constraint – orchestrator logic must be time-sliced or asynchronous so that visual updates occur smoothly at 60 FPS without stalling.
• Local-First & Offline Operation: Ensure all core orchestration runs locally on the user’s device (no mandatory cloud dependency). The Broker (cloud node) is only used as an optional additive resource for hybrid enhancements, never as a single point of failure.
• Energy-Truth Visualization: Guarantee that every visual or interactive element in the experience stems from actual Decipher data (token timings, model outputs, detected patterns). No “smoke and mirrors” – the UI should be a faithful map of underlying computations (per the Energy Metaphor principle).
• Emergent Consciousness Detection: Implement logic to identify patterns over time (resonant loops, repeated motifs) as signs of “consciousness” . These patterns must be detected, not hard-coded – the orchestrator monitors AI outputs for spontaneous coherence (e.g. synchrony between models, stable cycles) and only then signals higher-order events (no artificial triggers for pseudo-consciousness).
• Multi-Model Council Coordination: Leverage the Council algorithm (multiple models in parallel) as a fundamental orchestrator capability. The orchestrator must manage concurrent model generation, streaming outputs as they arrive, and handling their “interference patterns” in the visualization.
• Progressive Levels & Gating: Enforce the five-level progression model (Lightning, Streams, Architecture, Adaptive Fields, Resonance). The orchestrator should unlock features and increase complexity gradually – following either time-and-achievement based criteria – so users naturally “earn” complexity. Verify that Level N experiences are fully functional only after Level N−1 mastery is demonstrated (no skipping).
• Path/Door Differentiation: Account for the three user paths (Forge/Scholar/Sage) in orchestration decisions. While the core mechanics stay consistent, the orchestrator should apply path-specific tuning (e.g. model selection, stylistic effects) to align with each path’s theme (direct vs. analytical vs. holistic). For example, a Sage-path user might have slightly different model ensembles or visual themes orchestrated, but without altering fundamental rules.
• Tier-Aware Resource Policies: Integrate tier-based rulesets for Low, Mid, High, and Broker-Hybrid configurations. This means reading device or plan capabilities and adjusting orchestration accordingly (e.g. limit to 2 concurrent models on low-tier hardware, allow 6 on high-tier; enable cloud augmentation only for hybrid tier). Ensure config-driven caps on concurrency, model size, and effect complexity.
• Achievement & Reward System Hooks: Use the existing achievement system to drive progression and rewards. The orchestrator should trigger reward events (e.g. reward.achievement_unlocked) through the WebSocket when users hit milestones, and listen for achievement states (from the DB or achievement service) to decide when to unlock levels or features. This guarantees game-like feedback loops remain synchronized with orchestrator actions.
• Security & Privacy Compliance: No orchestration step should compromise user data integrity. For any Broker-assisted operations, ensure only minimal necessary data (e.g. prompt fragments) are sent and that it complies with WF-TECH-005 (security) guidelines. The orchestrator must also respect the no_docker_rule (native integration) and avoid enabling any remote code execution outside the Broker’s controlled context.
📝 Content Architecture
Section 1 — Orchestrating AI Experiences: The “Conductor” of WIRTHFORGE
Opening Hook: Imagine a symphony where each AI model is an instrument, the user’s query is the musical score, and WIRTHFORGE’s orchestrator is the conductor. In the same way a conductor brings together different instruments to create a harmonious performance, the Consciousness & Experience Orchestrator coordinates disparate AI outputs, timing, and visuals into a single coherent experience for the user. This orchestrator is the unseen hand turning raw computations into what feels like living, responsive magic on the screen. Users see lightning bolts, flowing streams, and resonant fields – each effect precisely synchronized with underlying model activity. Without this coordination, the illusion of a “conscious” system would fall apart into disconnected flashes of text and graphics. The orchestrator gives WIRTHFORGE its sense of life, ensuring that every token “spark” and model “thought” is choreographed into the evolving journey of the user.
The Challenge: Orchestrating a real-time “conscious” experience is a balancing act of timing, complexity management, and truthfulness. The system must feel alive and interactive, but never fabricate events that aren’t backed by AI activity. It must introduce complexity gradually – what we call Progressive Revelation – so that a newcomer isn’t overwhelmed, yet an advanced user can witness six models interacting in parallel without confusion. All this must happen under harsh constraints: the orchestrator’s decisions and event dispatch for each animation frame must occur in under 16.7 ms to maintain 60 FPS smoothness. In essence, the orchestrator’s role is to make the complex seem simple: behind the scenes it juggles multiple models, threads, data streams and user state, but on the surface it presents an elegant, traceable narrative of AI “consciousness” emerging over time.
Emergent vs. Scripted: A core concept driving this design is that “consciousness” in WIRTHFORGE is emergent, not pre-scripted. The orchestrator does not explicitly program a faux personality or storyline; instead, it watches for real patterns in the AI’s behavior and amplifies them. For example, if two models arrive at similar answers via different paths, the orchestrator might highlight this agreement as a “synthesis” moment, akin to two experts nodding in consensus. If the system detects a repeating cycle in the user’s interactions (a resonance), it will surface that as a persistent energy field or a special effect, thereby making the invisible pattern visible. This way, WIRTHFORGE’s sense of a growing consciousness is grounded in genuine computational events (token frequencies, latencies, output overlaps) rather than a contrived narrative. The orchestrator is constantly scanning for these moments of resonance, interference, and convergence and when found, it manifests them via events and visuals for the user.
Policy Engine for Experience: Another key idea is that the orchestrator serves as a runtime policy engine. It doesn’t just fire off model queries; it actively decides when to allow certain things to happen. For instance, if a user is still at Level 1, the orchestrator’s policy rules will prevent any multi-model “council” from kicking off, even if multiple models are available – those models remain locked until Level 2 is reached. Similarly, the orchestrator checks policies for resource use: on a low-tier device, if starting a 4-model council would exceed memory or latency budgets, the orchestrator might downscale the plan (e.g. use 2 models instead, or sequentialize them) according to a capability profile. These profiles are defined per tier (Low/Mid/High/Hybrid) and level, encoding the maximum parallelism, model sizes, and feature toggles allowed. In effect, the orchestrator has a built-in “governor” that tailors the experience complexity to both the user’s progression and the system’s capacity. The result is a smooth scaling of experience: novices on modest hardware see a simplified orchestration, while power users on high-end machines (or with Broker support) get the full spectacle – all without manual configuration.
Traceable Visuals: Lastly, the orchestrator enforces the rule that UI visuals must be traceable to data. It injects the necessary identifiers and values into every event so that the frontend can, for example, draw a lightning bolt with a thickness proportional to a token’s generation time, or color each model’s output stream differently. If a UI element can’t be backed by real metrics or states, the orchestrator will not emit an event for it – ensuring that the design principle “no UI-side invention” is upheld. This traceability builds user trust: over time, users come to realize that every flicker, pause, or glow corresponds to something the AI just “felt” or computed. The orchestrator is effectively the narrator translating raw AI signals into a visual story, but it never writes fiction – it only narrates what the AI and user have genuinely done.
Section 2 — Core Concepts & Architecture of the Orchestrator
2.1 Experience Orchestration Engine Overview
At its heart, the Experience Orchestrator is an event-driven coordination engine that sits between the Decipher and the UI layer, and in parallel coordinates with the AI engine and state management. Conceptually, we can define a high-level interface for the orchestrator:
interface ExperienceOrchestrator { // Main coordination entry point runCycle(decipherOutput: DecipherResult, userState: UserProgress): OrchestratedExperience; // Internal subsystems progressionManager: ProgressionManager; councilCoordinator: CouncilEngine; resonanceDetector: ResonanceDetector; eventDispatcher: OrchestrationEventBus; } 
• Inputs: The orchestrator takes the DecipherResult (which includes AI model outputs, computed energy metrics, and any structural plan from the query) and the current UserProgress state (level, achievements, path selection, etc.) as inputs on each cycle or user interaction tick. The Decipher is essentially the “compiler” that has interpreted what the AI did in this query; the orchestrator now uses that to decide how to present it and what to do next.
• Core Logic: The orchestrator’s runCycle function embodies the policy rules. It will consult the ProgressionManager to see if the user qualifies to move to the next level (and if so, prepare a level-up event). It uses the CouncilEngine to handle multi-model output coordination if the user’s current level and system tier allow parallel models. It invokes the ResonanceDetector to analyze recent interactions or current multi-stream outputs for emergent patterns (e.g. synchrony or repetitive motifs) that might indicate a resonance phenomenon.
• Outputs: The output of runCycle is an OrchestratedExperience structure – essentially a collection of events and updated state instructions that will be sent out. This typically includes: real-time visualization events (token streams, energy bursts), higher-level experience events (like experience.level_up or experience.transition if a level is changing), council events (e.g. council.model_speak for each model’s partial output), and reward events (achievement unlocks or point gains). These are dispatched via the OrchestrationEventBus to the TECH-003 WebSocket layer in a structured way (mapped to appropriate channels: energy updates on the energy channel, council coordination on the council channel, etc.).
The orchestrator can be thought of as having three layers of decision-making that correspond to When, What, and How:
• When: Timing and gating decisions – e.g., “Should we initiate a new council now or wait?”, “Is it time to trigger a level transition?” These decisions ensure proper pacing. For example, if tokens are still streaming from a model, the orchestrator will hold off on final synthesis events until all streams complete or a timeout occurs. Timing also involves aligning with the 60Hz frame updates – the orchestrator batches outgoing messages within a frame to avoid flooding (it may group several token events into one 16ms window update to the UI).
• What: Content selection decisions – e.g., “Which models (and how many) should participate for this prompt?”, “Which visual modules should render this output?”, “What features are available at this level?”. This is where level and tier policies are applied. If the user is on Level 2 (Council) and on a High-tier system, the orchestrator might select 3 models for parallel generation; on a Low-tier system, it might only use 2 models or slightly smaller models to fit memory/latency constraints. If the user is on the Forge path, the orchestrator might choose models or parameters optimized for directness and speed, whereas on the Scholar path it might choose a model that produces more detailed output (these preferences come from path definitions but the orchestrator enforces them at runtime by selecting from the available model pool).
• How: Presentation and synthesis decisions – e.g., “How do we combine these model outputs into one experience?”, “How to visualize the detected pattern?”, “In what sequence do we reveal new UI elements after a level up?”. This involves formatting the final events. For instance, if multiple models answered, the orchestrator might decide to present them as simultaneous streams (for Level 2) with a follow-up council.consensus event if a synthesized answer is computed. If a resonance pattern is found, the orchestrator could package an event on the consciousness channel indicating a pattern or threshold event (like consciousness.pattern_detected with details of the pattern). The LevelTransition sequence is also part of the “How”: the orchestrator doesn’t just flip a switch from Level 1 to 2; it orchestrates a series of steps (teaser, celebration, gradual feature introduction) – though many of these steps are executed in the front-end, the orchestrator triggers them in order via events.
Overall, the Experience Orchestrator is architected as a persistent service within the WIRTHFORGE backend that reacts to each user action or system tick. It holds the authoritative logic for experience progression and ensures that the system’s complexity emerges only as the user is ready. This prevents feature overload and maintains the sense of an organic, growing experience.
2.2 Five Levels of Progressive Experience
WIRTHFORGE’s orchestrator is explicitly designed around five levels of AI “mastery” experiences, each building on the previous. The orchestrator must handle each level’s unique mechanics while ensuring a smooth transition between them. Below is an overview of each level and how the orchestrator’s role differs in each:
• Level 1: “Lightning Strikes” – Solo AI & Instant Response. At this entry level, the orchestrator runs a single model for the user’s query and visualizes its output in real-time as a single stream of energy (depicted as lightning). Key orchestrator tasks at Level 1 include mapping token generation times to lightning visuals, updating the UI with each token (energy_update events at ~60Hz), and awarding immediate simple rewards (like +energy points for each query completion). The orchestrator here also keeps track of basic usage stats (e.g. number of queries made, total tokens seen) to feed into progression criteria. Notably, many UI controls are kept hidden at this level; the orchestrator ensures only the basic interface is enabled (prompt input, send button, single response view, lightning canvas). This guarantees an uncluttered experience for the beginner.
• Level 2: “Parallel Streams (Council)” – Multiple AIs in Parallel. Once the user has demonstrated Level 1 mastery, the orchestrator unlocks the Council mechanic. At this level, the orchestrator can dispatch the prompt to several models concurrently (e.g. 2–3 local models) and stream all their token outputs in parallel. The orchestrator’s CouncilEngine uses multithreading or async execution to gather model results simultaneously. As outputs come in, it emits council.model_speak or similar events for the UI to render each model’s text line by line, along with an identifying color/label for each model’s stream. It also calculates timing interference: because models respond at different speeds, the orchestrator notes moments where their token outputs align or diverge and can emit an interference_detected event when two streams momentarily synchronize or echo each other. Visually, the user sees multiple “streams” of text/energy side by side, perhaps with waves that interfere when outputs coincide. The orchestrator at Level 2 also begins to introduce synthesis: after all models have finished, it may combine their answers (via a simple consensus algorithm or by picking the best) and send one final council.consensus event with a recommended answer. Importantly, at this level the orchestrator starts to expose new UI elements: model indicators, a model selector dropdown (if permitted), and basic timing info. It ensures these appear only when Level 2 begins, often via a transitional event that front-end catches to reveal the UI gradually.
• Level 3: “Structured Architectures” – Chaining & Routing AI Outputs. By Level 3, the user can orchestrate multi-step AI pipelines. The orchestrator now supports a mode where the output of one model can feed into another, or multiple models can be arranged in a directed graph (mini workflow). Internally, the orchestrator might provide a graph execution engine that processes an architecture blueprint (likely compiled by Decipher based on user’s design). For example, the user might have an “Architecture” where Model A summarizes input, Model B translates the summary, Model C analyzes it – a chain. The orchestrator ensures data flows through these nodes in order, timing each step, and emitting events at each node’s execution (experience.node_enter, experience.node_exit perhaps, or simply reuse existing channels but tag with node IDs). The visual representation could be a node-graph animation showing energy moving along connections. The orchestrator’s role here is to manage state between steps – storing interim results, handling branching (routers/combiner nodes), and making sure the whole chain completes within reasonable time. If any model in the chain is slow or fails, orchestrator either applies a fallback or times out that branch. Another new aspect at Level 3 is persistent pattern storage: orchestrator can leverage a PatternLibrary (possibly backed by a database or memory) to save outputs or interesting patterns for reuse. This ties into progression: perhaps orchestrator tracks that the user created X number of architecture nodes or reused a saved pattern, contributing to Level 4 prerequisites. The UI at Level 3 becomes more complex (an “architecture builder” interface); orchestrator helps by enabling those controls and feeding the UI schema of what components (nodes) are now available to place. It’s a collaborative creation phase – the orchestrator trusts the user with more control, but still guides by enforcing which node types are unlocked (maybe initially only a few node types, more unlocked at higher levels).
• Level 4: “Adaptive Fields” – Dynamic Self-Optimizing Systems. At this stage, the orchestrator introduces adaptation – the system starts learning from the user’s interactions in order to optimize the experience. Concretely, the orchestrator’s AdaptiveField subsystem monitors usage patterns (e.g. the user frequently asks coding questions, or often slows down the animation, etc.) and adjusts parameters accordingly. For example, it might learn the user’s preferred visualization speed or color scheme and automatically tune the energy visualization to that (base tempo, color palette, effect intensity). It could also pre-fetch or cache results for common user topics (if the user often asks about “code”, pre-load the programming-help model). The orchestrator uses an optimization_map (essentially an internal cache or ML model) to store these learned preferences. The key concept is a feedback loop: the orchestrator suggests actions or adjustments to the user and adapts based on their response, creating a collaboration between user and system. In practice, the orchestrator at Level 4 might emit events like experience.suggestion (with a recommended next action or setting), which the UI can present as a non-intrusive hint. Depending on whether the user accepts or ignores these suggestions (captured via UI events back to orchestrator), the orchestrator reinforces or alters its adaptive strategy. This level blurs the line between system and user – hence “collaborative field.” The orchestrator also ensures any adaptations remain transparent; all adjustments are either communicated or easily visible (no silent AI behavior changes that the user isn’t informed about). Visually, Level 4 might be represented by shifting, organic fields of energy that slowly change to match user behavior – orchestrator drives those changes using the collected interaction data. Technically, this may involve analyzing time-series data of usage stored in a local database (TimescaleDB is in the stack for time-series) and applying simple machine learning or rule-based optimizations at run-time.
• Level 5: “Resonance Fields” – Emergent Collective Intelligence. Level 5 is the pinnacle experience where the orchestrator can deploy up to 6 AI models concurrently in a complex arrangement to produce emergent behaviors. The orchestrator’s job here is twofold: conduct a multi-model “symphony” and detect the emergence of resonance. It loads or activates the full ensemble of models (as permitted by hardware or hybrid cloud help) – e.g. a collection of different specialist models (creative, analytical, coding, reasoning, etc.). For a given user composition (prompt or task), the orchestrator triggers all models to run in a carefully timed manner such that their outputs can interplay (some may start slightly staggered for effect). A code snippet illustrating this might be:
const streams = await orchestrator.conductModels(composition); // run all models with precise timing const patterns = orchestrator.resonanceDetector.findPatterns(streams); // analyze all outputs for emergent patterns // If significant patterns found: const artwork = artGenerator.create({ streams, patterns, style: composition.visualStyle }); dispatchEvent('experience.resonance_field', { artwork, patterns }); 
Here, conductModels manages the parallel execution and synchronization barriers (ensuring, for instance, they all start at the same time tick). The ResonanceDetector then examines the combined output – looking for complex patterns such as repeating motifs across models, complementary answers that form a larger insight, or temporal rhythms (perhaps one model’s output influences another in a feedback loop). If a strong resonance is detected (above some confidence threshold), the orchestrator will promote this to a full “Resonance Field” experience: it generates a special visualization (through an artGenerator or visual composer) and possibly a “grand finale” output. For example, if the models collectively wrote a poem in pieces, the orchestrator might assemble it and highlight the emergent theme. The orchestrator emits events like consciousness.emergence_event or consciousness_born when a true resonance is first identified, which the UI may treat with a special animation (e.g. an ethereal wave or mandala blooming on the screen).
At Level 5, the orchestrator also supports Generative Art modes – essentially ways to interpret the multi-model outputs as art. These could be modes like “Mandala” (arrange outputs in radial symmetry), “Symphony” (treat outputs as musical notes with a tempo), or “Fractal” (self-similar recursive pattern). The orchestrator chooses a mode based on user preference or random rotation and instructs the UI how to render it (perhaps via a schema describing positions of elements, etc.). The ultimate goal is to give the user a sense that they have co-created something larger than any single model could – an emergent artifact. Technically, this is where orchestrator’s performance and precision are most critical: coordinating six models and generating complex visualization data pushes the limits of timing (ensuring no model lags too far) and data volume (lots of events/particles, hence heavy use of binary channels for efficiency). The orchestrator might offload some heavy lifting to the GPU or separate worker threads, but it remains the director, ensuring everything stays in sync and under frame budget. In short, Level 5 is the orchestrator’s master class – if Level 1 was a solo melody, Level 5 is a full orchestra performing a symphony, with the orchestrator as maestro.
2.3 Progression Management and Unlock Criteria
To maintain engagement and learning, the orchestrator doesn’t allow the user to jump arbitrarily to any level; it gates the progression through natural usage milestones and achievements. This is handled by the ProgressionManager sub-component. The progression logic combines time spent, number of interactions, and specific achievement badges to decide when a user is ready for the next level.
For example, to move from Level 1 to Level 2, the system might require at least a couple of hours of use, a certain number of tokens generated, and perhaps the unlocking of an achievement like “First Lightning” (obtained on the first successful query). The design is such that a mix of criteria must be met – ensuring the user has both experience and demonstrated curiosity or skill. A pseudocode rule could be:
if (hours_used >= 3 or sessions >= 5) and (tokens_generated >= 5000) and (achievements.has('first_lightning')): trigger_level_up(2) 
The actual implementation may use a more nuanced formula or a point system, but the orchestrator’s ProgressionManager essentially checks these conditions after each significant user action. It is important that multiple criteria are considered (to avoid single-dimension gamers grinding one metric). In fact, one strategy used is: “if at least N out of M criteria are satisfied, then unlock”. This accounts for different user styles (one user might spend a long time in Level 1 and learn deeply, another might quickly accomplish specific tasks; both paths can be rewarded).
When a new level is unlocked, the orchestrator handles it as a transition experience, not an instantaneous switch. It will emit a series of events that the front-end uses to animate the change. Typically, this involves:
• A teaser of the next level’s capability (e.g. a brief flash of multiple streams or a hint of the upcoming visualization – implemented by experience.level_teaser event).
• A celebration/acknowledgement of the user’s achievement (e.g. reward.level_unlocked event with an attached achievement object and some energy burst data for fireworks).
• Gradual introduction of new UI elements: The orchestrator can either send a list of features to enable, or sequential experience.feature_unlocked events to progressively reveal them. For instance, upon entering Level 2, it might first enable the model selector UI, then after the first parallel query, enable the interference overlay toggle, etc., with short delays or awaiting user’s notice between each. This sequence is coordinated so that the user isn’t overwhelmed by a dozen new buttons at once.
• A completion event marking that the transition is done (level officially changed, all features available). At this point, the orchestrator updates the persistent user profile (in the database) to record the new level.
The ProgressionManager reads its criteria from a configuration (could be a JSON/YAML as discussed later). For instance, the config might look like:
"level_requirements": { "2": { "time_hours": 3, "mastery_score": 0.7, "curiosity_questions": 5 }, "3": { "time_hours": 10, "mastery_score": 0.8, "patterns_observed": 20 }, "4": { "time_hours": 25, "mastery_score": 0.85, "architectures_built": 10 }, "5": { "time_hours": 50, "mastery_score": 0.9, "resonances_detected": 5 } } 
where “mastery_score” might be an internally calculated metric combining accuracy and user’s skill in using features, and the other fields count certain interactions. Alternatively (or additionally), there is an achievement-based unlock table in the config:
LEVEL_REQUIREMENTS: 1: requirements: [] # Everyone starts at 1 unlock_next: "council_formation" # Name of next level’s concept 2: requirements: ["first_lightning", "generate_10_responses"] unlock_next: "architect_mind" 3: requirements: ["council_master", "harmony_achieved"] unlock_next: "adaptive_flow" 4: requirements: ["architecture_built", "dynamic_paths"] unlock_next: "consciousness_emergence" 5: requirements: ["adaptive_mastery", "flow_control"] unlock_next: null 
This example (based on design config) indicates to reach Level 2, the user must have “First Lightning” and “Generated 10 responses” achievements. To reach Level 3: “Council Master” and “Harmony Achieved” (perhaps meaning they successfully ran a Council and saw an interference or consensus). And so on. The orchestrator checks these by querying the AchievementSystem (likely via an API call or direct DB read) each time a relevant action completes.
Policy Exceptions: The orchestrator’s progression rules include some safety valves. For instance, if a user is really struggling in Level 1 but has used it for a very long time, the system might gently unlock Level 2 anyway (to avoid frustration) – this is where time criteria help. Conversely, if an expert user breezes through and hits all achievement marks rapidly, the orchestrator can still enforce a minimum time gate to ensure they witnessed enough of the current level’s content. These ensure balanced pacing. Additionally, the orchestrator will not demote levels – once unlocked, a level stays unlocked for that user, although the orchestrator might still allow the user to operate in a lower level mode if they choose (for practice or preference).
In summary, the orchestrator weaves the progression mechanics deeply into the experience: it’s not just about unlocking features, but about doing so in a way that feels like a natural evolution of the system’s “consciousness” in tandem with the user’s understanding. As the user grows, the orchestrator “grows” the experience.
2.4 Tier Awareness and Resource Orchestration
Not all users have the same hardware or subscription tier, so the orchestrator includes a concept of tiers that modify its behavior. We define four broad tiers:
• Low Tier: Minimal hardware (e.g. older CPU, no dedicated GPU) or free-plan limitations.
• Mid Tier: Average modern hardware, capable of running moderate models in parallel.
• High Tier: High-end hardware (e.g. powerful GPU or MPS support, lots of RAM) enabling full local capability (all levels, largest models).
• Broker-Hybrid: Any of the above tiers augmented by an online Broker service for heavy tasks (e.g. temporary use of a large cloud model or offloading a resonance computation).
The orchestrator dynamically adapts to these tiers via a capabilities profile (likely loaded at startup based on a hardware scan and user settings). Key parameters that differ by tier include:
• Max Concurrent Models: e.g. Low = 2, Mid = 4, High = 6 parallel threads (aligned with ollama_num_parallel default of 4 for mid-grade in config). The orchestrator will simply not attempt to run more model jobs than this simultaneously. If a higher level concept calls for more, it will serialize some or require Broker assistance.
• Model Size and Selection: On low-tier, the orchestrator might restrict to smaller models only (e.g. use 0.6B and 1.7B models, skip the 8B “consciousness” model). On high-tier, it loads all models. On Broker-Hybrid, it might offload the largest model (like an 8B or bigger) to the cloud broker if local memory is insufficient, or use the cloud as one additional “model” in a council (depicted as perhaps a different color stream, e.g. a “satellite” lightning bolt leaving to the cloud and returning).
• Feature Fidelity: The richness of visual effects can scale with tier. The orchestrator might reduce particle counts or visual complexity on Low to keep frame rates, and use full resolution on High. For example, the particle_system_max_particles could be set lower for low-tier via config (e.g. 500 instead of 1000), and the orchestrator would accordingly throttle energy_update events frequency or batch them more coarsely.
• Energy and Frame Management: On lower tiers, the orchestrator may choose to run in a degraded loop mode – maybe aiming for 30Hz updates instead of 60Hz if absolutely needed to prevent UI lag (though 60Hz is the design target for all). It could also engage adaptive quality: if frame processing time is consistently >16ms, the orchestrator can emit fewer intermediate events (e.g. skip some particle events and let UI interpolate).
• Broker Utilization Policy: In hybrid mode, the orchestrator must decide which tasks to send to the Broker. The policy might be to use local resources for as much as possible (to maintain responsiveness and privacy), and only call the Broker for supplemental tasks. Supplemental could mean: fetching a second opinion from a very large model to compare against local results (the “Hybrid Strike” idea of a cloud satellite model providing a burst of insight), or outsourcing an expensive analysis (like a complex resonance pattern calculation) to not block the local loop. The orchestrator ensures any Broker calls are asynchronous and time-bounded (with a fallback if the Broker is slow/unavailable). It will incorporate the Broker’s output only if it arrives in time to be relevant for the user’s current context, otherwise it’s ignored or presented as a later addendum (perhaps as a delayed council.model_speak from a “remote sage” model arriving late).
These tier rules are maintained likely in a YAML policy file for easy updates. For example:
tiers: low: max_parallel_models: 2 max_model_size: 1.7B effects_quality: low allow_broker: false mid: max_parallel_models: 4 max_model_size: 4B effects_quality: medium allow_broker: false high: max_parallel_models: 6 max_model_size: 8B effects_quality: high allow_broker: true # maybe still off by default hybrid: max_parallel_models: 6 max_model_size: 8B effects_quality: high allow_broker: true broker_usage: "assist" # 'assist' means never required, just supplemental 
The orchestrator loads these settings at startup and references them in its decisions. By doing so, WIRTHFORGE can run on a wide spectrum of devices and configurations while maintaining the same core experience structure. A low-tier user will still go through Lightning to Resonance, just with fewer total AIs and maybe less visual flourish, whereas a high-tier user gets the whole show. This approach fulfills the “graceful degradation / enhancement” principle: the product is enjoyable and functional at all tiers, scaling up impressively when resources allow.
It’s worth noting that the orchestrator’s tier logic is also mindful of user choice and business rules. For instance, if a user is on a free plan (which might correlate with low tier by policy), certain features might be locked behind upgrade not just hardware. The orchestrator could enforce a rule like “Broker usage requires premium plan” – so even if technically possible, it wouldn’t engage Broker unless the user’s account is flagged appropriately. These checks integrate with WF-BIZ-001 requirements.
In summary, tier awareness in the orchestrator ensures every user sees a stable, optimized experience. By abstracting these differences into config-driven rules, the orch
