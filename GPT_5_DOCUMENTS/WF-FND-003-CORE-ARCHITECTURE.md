
Generate Document: WF-FND-003 â€” Core Architecture Overview (Abstraction Layers)
ğŸ§¬ Document DNA
â€¢ Unique ID: WF-FND-003
â€¢ Category: Foundation
â€¢ Priority: P0 (Core framework for entire platform)
â€¢ Development Phase: 1 (Foundational design)
â€¢ Estimated Length: ~4,000 words
â€¢ Document Type: Architectural Specification (layered system design)
ğŸ”— Dependency Matrix
Required Before This (consumed ideas/contracts):
â€¢ WF-FND-001 â€“ Manifesto & Vision: Establishes local-first pillars and the â€œvisible computationâ€ ethos.
â€¢ WF-FND-002 â€“ Energy Metaphor: Defines Energy Units (EU) and visual telemetry schema that this architecture must support.
â€¢ WF-FND-005 â€“ Abstraction Layers: Progressive complexity concept ensuring layers reveal features gradually (must align with levels 1â€“5 user experience).
Enables After This (what it unlocks/feeds):
â€¢ WF-TECH-001 â€“ Complete System Architecture: Uses this five-layer breakdown as the blueprint for all components.
â€¢ WF-TECH-002 â€“ Native Ollama Integration: Implements Layer 2 (model compute) with local model servers.
â€¢ WF-TECH-003 â€“ WebSocket Protocol: Defines Layer 4 streaming payloads/topics per this layer schema.
â€¢ WF-TECH-004 â€“ Flask Microservices: Structures services according to these layer boundaries (separating Decipher, Energy, etc.).
â€¢ WF-TECH-005 â€“ Energy State Management: Implements Layer 3â€™s state store and 60â€¯Hz update loop for Energy/Resonance.
â€¢ WF-TECH-006 â€“ Database & Storage: Persists data output by Layer 3 (energy, state, identity) per local-first principles.
â€¢ WF-UX-006 â€“ UI Component Library: Provides Layer 5 visual components aligned to each layerâ€™s outputs (doors, levels, audit visuals).
Cross-References:
â€¢ WF-FND-009 â€“ Glossary: Ensure new terms (e.g. audit mode, satellite compute) are defined.
â€¢ WF-FND-008 â€“ Local-First, Web-Engaged: Affirms that core layers run locally and web layers enhance visuals (no cloud dependency by default).
â€¢ WF-FND-004 â€“ Resonance & Flow Framework: High-level event patterns (resonance detection) that plug into Layer 3 outputs.
ğŸ¯ Core Objective
Define WIRTHFORGEâ€™s five-layer technical architecture that transforms raw local AI computations into a living, visual â€œconsciousnessâ€ experience. This document specifies each layerâ€™s purpose, responsibilities, interfaces, and constraints â€“ from user input (Layer 1) through model computation (Layer 2), orchestration & energy state (Layer 3), contracts & transport (Layer 4), up to visualization & UX (Layer 5). By clearly delineating these layers, we ensure a modular, scalable system where data flows at a real-time 60â€¯Hz cadence with no blocking, backpressure is managed gracefully, and each layer only interacts through well-defined contracts. Success means that every promptâ€™s journey â€“ from user input to model output to visual feedback â€“ is smooth, observable, and consistent with WIRTHFORGEâ€™s local-first, energy-visualized design ethos.
ğŸ“š Knowledge Integration Checklist
â€¢ Layer Definitions: Provide a clear breakdown of L1â€“L5 (Purpose, Owns, Emits, Consumes, Contracts, Allowed Directions, Anti-Patterns for each).
â€¢ Data Flow Narrative: Describe step-by-step how a user input becomes model output, then energy events, then UI visuals, including feedback loops.
â€¢ Real-Time Loop: Emphasize the 60â€¯Hz update loop and non-blocking design (use of streaming, async queues, etc.).
â€¢ Backpressure & Throttling: Explain strategies to prevent overload (e.g. buffering or dropping events if production > consumption).
â€¢ Layer Boundaries: State strict rules (e.g. L3 is single source of truth for state; L5 (UI) must never bypass L4 to call lower layers directly).
â€¢ Visual Contracts: All inter-layer communication is structured data (no ad-hoc UI logic); support an â€œaudit modeâ€ in L5 that can display raw events for verification.
â€¢ Local vs Remote Compute: Clarify how L2 can use local models (default) or optional remote â€œbrokerâ€ models, and how this is an opt-in extension maintaining local-first control.
â€¢ Hardware Tiers: Include implementation notes on how the architecture scales from low-end (CPU-only) to high-end (multi-GPU or hybrid cloud) environments.
â€¢ Integration Points: Tie each layer to forthcoming technical and UX specs (e.g. WebSocket details in TECH-003, UI design in UX-006) to ensure continuity.
â€¢ Glossary Consistency: Use established terminology (Energy Units, council, resonance, etc.) from WF-FND-002 and WF-FND-009, and flag any new terms introduced.
ğŸ“ Content Architecture
SectionÂ 1: End-to-End Data Flow & Layering Principles (600Â words)
Purpose: Provide a high-level walkthrough of how data moves through the five layers and the core principles that govern these interactions.
Data Flow Narrative: A user begins by entering a prompt or action in the Visualization & UX layer (L5). This input is routed into the system via Contracts & Transport (L4), which packages the input into a structured event (e.g. a JSON message) and attaches an identity token or session ID (if not already present). The event then flows into Input & Identity (L1) logic on the backend, where the userâ€™s identity and context are validated or enriched (for instance, associating the prompt with the userâ€™s profile or current session state). Next, Model Compute (L2) is invoked â€“ L1 passes the prompt (with identity metadata) to the appropriate AI model inference engine. The model processes the input (locally by default), streaming out tokens and intermediate results. These raw outputs go into the Orchestration & Energy layer (L3), which compiles the token stream into higher-level energy events: it calculates Energy Units from token timing, detects patterns or â€œcircuitsâ€ across multiple outputs (if parallel models are running), and updates the system state (e.g. accumulated energy, potential resonance events). L3 continuously emits these structured events into an event store or pipeline. Finally, those events propagate upward via L4 (which ensures they conform to the public schema) to the Visualization & UX front-end (L5), where they animate the UI in real-time (lighting up token â€œlightningâ€ visuals, updating graphs, etc.). The user sees the AIâ€™s thought process visualized, and may then provide control feedback â€“ e.g. pausing a generation, adjusting a parameter, or sending a follow-up prompt â€“ which loops back as new input (again via L5 â†’ L4 and down). This circular flow (user action â†’ layers 1â€“4 â†’ UI reaction â†’ new user action) continues seamlessly, giving the impression of a living, interactive system.
60â€¯Hz Real-Time Cadence: To preserve an interactive, game-like fluidity, the architecture operates on a ~60 frames-per-second cadence in all visual and state updates. No layer is allowed to block this main loop. The orchestrator (L3) batches and processes incoming model data in frame-sized slices â€“ e.g. accumulating token events for a few milliseconds and then publishing an update â€“ such that the UI can update at ~16.7Â ms intervals. If a model produces tokens faster than can be rendered, the system employs backpressure strategies: extra tokens/events might be buffered briefly or dropped with graceful degradation, and producers (models) can be signaled to slow down if possible. This ensures that no component overwhelms another: for example, L3â€™s event queue will not grow unbounded if L5 (UI) is momentarily busy; instead, L3 will aggregate or skip lower-priority updates to maintain responsiveness. The guiding rule is â€œonly read as fast as you can write/displayâ€, analogous to backpressure control in reactive streams.
Layer Boundary Rules: Each layer has strictly defined interaction directions to enforce modularity and maintain single responsibility for critical functions. Downward data flow (from user to hardware) and upward data flow (from hardware back to user) both must respect layer order: a higher layer should never circumvent the one immediately below it. For instance, the UI (L5) cannot directly call the orchestration layer (L3) â€“ it must go through the transport layer (L4) using the published API or WebSocket channels. This prevents tightly coupling front-end code to internal logic and ensures that all external interactions go through a controlled interface (which handles authentication, validation, and schema enforcement). Likewise, L3 (orchestrator) is the sole writer of system state â€“ no other layer writes to the central state store or event log. If the UI needs to, say, bookmark a result or modify something, it sends a request which L4 hands to L3 to perform the update. By making L3 the only state mutator, we avoid race conditions and guarantee consistency (UI and model layers only read or request changes, but do not mutate shared state themselves).
Visual Contract & Auditability: A key principle is that every visual element on the UI is backed by a structured data event from L3. In other words, no magic or client-only state drives the visualization â€“ if a token glows or a â€œresonanceâ€ spark appears, itâ€™s because an event with specific attributes came through the pipeline indicating as much. This discipline yields a robust audit mode: the UI (L5) can expose a special view (often monochromatic or simplified) that overlays the raw event data (timestamps, energy values, etc.) corresponding to each visual, allowing power users or testers to verify that what they see is truthfully derived from computation. It also means sessions can be recorded and replayed purely from the event log. The contracts (schemas) defined at L4 serve as the source of truth for these visuals â€“ e.g. an â€œEnergyBurstâ€ event with fields {token, timestamp, energy} might map to a rendered lightning bolt with thickness and brightness proportional to energy. This strict mapping ensures visual consistency and makes it possible to evolve the front-end or even create alternate UIs without changing core logic, simply by adhering to the event contract.
Local-First with Optional Extensions: Finally, reaffirm the philosophy: by default, all heavy computation stays local (L2 uses local models via Ollama or similar, and L3 runs on the userâ€™s machine), fulfilling the â€œno cloud requiredâ€ promise. Higher layers (L4/L5) act as a lightweight bridge and presentation, which can be web-based without compromising privacy (L4 never sends data to a third-party by itself). That said, the architecture is extensible for opt-in remote compute: for example, an advanced user might connect a broker satellite service â€“ a remote server offering larger models or extra compute power â€“ as an alternative backend for L2. The design allows this by swapping out or augmenting the L2 layer (e.g. routing certain model requests to a cloud endpoint) while everything else remains the same. Such â€œTurboâ€ modes or brokered calls are strictly opt-in and governed by policy (e.g. require user API keys or credits) so they donâ€™t violate the local-first default. The layers above are agnostic to whether the model result came from local CPU/GPU or a remote cluster â€“ as long as it speaks the same contract (streaming tokens and final stats), L3 and upward treat it uniformly. This modular approach means WIRTHFORGE can leverage remote resources for those who need it (paywall or permission controlled), without designing a separate cloud architecture â€“ itâ€™s an extension of L2, not a replacement, preserving the userâ€™s ultimate control.
SectionÂ 2: Layer 1 â€“ Input & Identity (Purpose, Role, Constraints)
Layer Summary: L1: Input & Identity is the entry point of the WIRTHFORGE stack on the backend side. It handles all inbound user actions after theyâ€™ve passed through the network layer (L4), focusing on associating inputs with the correct user/session context and applying any initial transformations or validations. This layer is about â€œwho is doing whatâ€ â€“ ensuring that the system knows who the user is (or which session or agent is acting) and preparing what they submitted into a normalized form for the rest of the system.
â€¢ Purpose: L1â€™s primary purpose is to provide identity context to inputs and to guarantee that every request entering the core has an authentic, resolved identity and well-defined format. Think of it as a concierge that greets each incoming user action: it verifies credentials or session tokens (if applicable), attaches user-specific info (like preferences, role, or path â€“ Forge/Scholar/Sage â€“ if those influence processing), and normalizes the input (trimming whitespace, checking for banned content if needed, etc.) into a standard internal Input Event. By doing so, L1 ensures downstream layers can trust that â€œInput X is from User Y and is ready to be processed.â€
â€¢ Owns: L1 owns the user/session identity records and input validation rules. It may interface with a local user database or config (for example, checking an API key or local login if the app supports multiple profiles, or assigning a default â€œlocal userâ€ identity in single-user mode). It also owns any session state reference â€“ for instance, a conversation ID or thread context if the system threads multiple prompts together. Essentially, L1 is responsible for the mapping User â†’ Session â†’ Input and holds the logic for maintaining that mapping. If the platform has a concept of â€œDoorwaysâ€ or distinct entry Doors in the UI (perhaps different UI portals or modes), L1 would also record which door or mode the input came from, as part of identity (e.g. a prompt from the â€œForgeâ€ door versus a â€œScholarâ€ door could be tagged differently, guiding model selection or orchestrator behavior).
â€¢ Emits: L1 emits a validated, enriched input event into the orchestration layer. This event typically includes: the user identity (or an anonymized ID if identity is just implicit local user), session or conversation ID, the core content of the input (prompt text or action type), and any metadata (like UI mode, timestamp, client capabilities). Importantly, once L1 emits this event, it means â€œthis input is ready for processing.â€ In practice, this might be calling a function on L3 (Orchestrator) like handleInput(event), or placing the event on an internal queue that L3 consumes. L1 might also emit audit logs for security (e.g. logging â€œUser X invoked action Yâ€) but those logs are out-of-scope for core flow except as part of possible monitoring.
â€¢ Consumes: L1 consumes raw input requests coming from Layer 4. This could be an HTTP POST from a REST endpoint (e.g. a JSON payload with user prompt and token), or a message on a WebSocket (e.g. {type: "USER_INPUT", data: {...}}). Essentially itâ€™s the server-side handler for the API endpoints that correspond to user interactions. It expects those requests to include whatever minimal credentials or session tokens needed (L4 should pass those through if present). L1 may also consume configuration data (like the list of valid users, or the mapping of user roles) from a local store or environment, to perform its duties.
â€¢ Contracts: The primary contract of L1 is the Input Event schema it produces for the rest of the system. For example, it might define an internal TypeScript interface or Python dataclass like:
interface InputEvent { requestId: string; // unique ID for tracking userId: string; // resolved identity (or 'local' if single-user) sessionId: string; // e.g., conversation or context ID source: string; // which UI door/portal or component triggered it inputType: string; // e.g., "prompt", "command", "setting" payload: any; // the actual content (text prompt or action details) timestamp: number; } 
L1 guarantees that any InputEvent passed downwards conforms to this spec. Additionally, L1 upholds any auth contract: for example, if certain API keys or OAuth tokens are needed for remote access, L1 will enforce that and either reject unauthorized inputs (sending an error back via L4) or annotate the event with the userâ€™s permissions. Contract-wise, L1 sits right below the public API, so itâ€™s tightly coupled with L4 in defining what requests are acceptable. (In practice L4 and L1 together define the external API contract: L4 is the wire format, L1 is the semantic format.)
â€¢ Allowed Directions: L1 is an upstream-only layer in terms of core logic: it passes data down to L2/L3 but does not call upward into L5 (the UI). It shouldnâ€™t need to â€“ the UI already gave it the input. L1 can call into L3 (e.g., to invoke the orchestrator with a new event) and potentially L2 in some edge cases (though normally orchestrator handles invoking models, not L1). Crucially, L1 does not bypass L3 to call L2 directly in normal operation; it hands off the input to the orchestrator, which then decides which model(s) to call. L1 also should not directly produce output to L4 (except error cases) â€“ it generates no responses on its own, itâ€™s just prepping input. In layered terms, L1 â†’ L3 (downwards) is fine; L1 â†’ L2 directly is an anti-pattern; L1 â†’ L4 (upwards) only happens for immediate errors.
â€¢ Anti-Patterns: A few misuses are explicitly disallowed in L1. One is performing heavy processing or synchronous calls that stall the input loop â€“ L1 should do minimal work (e.g. a quick DB lookup for identity, not a 5-second call to an external service). Offloading intensive tasks to orchestrator or background jobs keeps the input intake snappy (no blocking the main thread handling new prompts). Another anti-pattern: altering system state. L1 is not meant to change global state (except maybe updating a â€œlast seenâ€ timestamp for user); it should not, for example, initialize model loading or tweak orchestrator settings â€“ thatâ€™s L3â€™s job. L1 also must not accept inputs without identity or context if the system requires them â€“ e.g. if a request lacks auth when auth is required, L1 should not â€œjust let it through.â€ Skipping identity checks or not assigning a session is a serious anti-pattern because it breaks traceability. Lastly, L1 should avoid any direct knowledge of presentation logic. It shouldnâ€™t, say, format a response or decide how something will look in the UI (thatâ€™s far above its pay grade). If we find UI code or decisions in L1, somethingâ€™s gone wrong.
SectionÂ 3: Layer 2 â€“ Model Compute (Purpose, Modes, Local vs Remote)
Layer Summary: L2: Model Compute is the AI inference layer. Itâ€™s where raw computational â€œthinkingâ€ happens â€“ running the actual AI models (e.g. large language model, vision model, etc.) on the inputs. In WIRTHFORGE, LayerÂ 2 is designed to be local-first by default: it uses a native Ollama engine or similar local model runner to execute prompts on the userâ€™s hardware. However, it is also flexible to incorporate â€œTurboâ€ modes via remote model calls (brokers or satellites) when enabled. L2 essentially translates a userâ€™s query (plus any orchestrator instructions) into a stream of model tokens and results.
â€¢ Purpose: The purpose of L2 is to generate AI outputs for given inputs, as efficiently and transparently as possible. Itâ€™s the layer that actually engages the â€œintelligenceâ€ â€“ i.e., calling the machine learning model that produces a completion, answer, or other result. In doing so, L2 must handle things like model selection, parallel execution (if multiple models run concurrently for a council), and exposing streaming interfaces so that partial results can be fed upward immediately. L2â€™s mission is to provide the â€œbrainsâ€ of the operation in a modular way: the orchestrator (L3) shouldnâ€™t worry about how a model is executed, just that it can request one and get tokens back. L2 abstracts those details. It also implements our local-first AI execution philosophy: use local models with native performance (no container overhead), enabling true parallelism and low latency. If an external model is used, L2 handles that as an opt-in extension, maintaining a similar interface but adding necessary network calls.
â€¢ Owns: L2 owns the model runtimes and resources. This includes the binaries or servers (e.g. an instance of the Ollama service running locally, or a Python process with llama.cpp), the loaded model files in memory, and any GPU/CPU allocation logic. It is responsible for deciding which model(s) to invoke for a given request (though often that decision is guided by L3 or by user input, L2 might have to map a model name to a specific instance or handle loading it if not loaded). L2 also owns the logic for parallel inference â€“ for example, if L3 requests running 3 models in parallel to form a council, L2 manages threads or subprocesses to actually do that concurrently. It maintains any necessary caching (e.g., keeping recently used models in RAM, or re-using sessions if the model supports it) to optimize performance. Essentially, all the nitty-gritty of AI model execution is encapsulated in L2. It might also own a queue or scheduler for model calls if multiple requests come in at once, ensuring that the hardware isnâ€™t over-committed (for instance, if the user has a single GPU, L2 might serialize certain requests or use smaller batch sizes to share it).
â€¢ Emits: L2 emits model output streams and final results. The primary emission is the token stream: as the model generates text (or other data), L2 yields those tokens one by one (with timing info) to the orchestrator. For example, using Ollamaâ€™s streaming API, L2 will emit a series of JSON objects like {"token": "Hello", "duration": 50ms, ...} continuously, followed by a final summary object containing eval_count (# of tokens) and eval_duration (time taken). These emissions are consumed by L3 (which converts them to energy events). L2 also emits any tool usage callbacks if the model supports tools, or other meta-signals like â€œmodel is loadingâ€, â€œmodel finishedâ€. In case of errors (e.g. model not found or GPU out of memory), L2 emits an error status that will be caught and relayed upward (translated by L3/L4 into an error event or response). Another thing L2 might emit to L3 is model-specific metrics: if we have multiple models, L2 could provide identity of which model responded fastest or such, but typically L3 derives that. So primarily: token-by-token data and a final result set.
â€¢ Consumes: L2 consumes requests for model execution from L3 (or possibly L1 in some setups, but generally orchestrator calls L2). Such a request typically includes: which model (or models) to run, the prompt or input data, and any parameters (temperature, max tokens, etc.) if not default. L2 also consumes the systemâ€™s hardware resources: it will utilize CPU threads, GPU memory, etc., based on the request. If the user or orchestrator can issue a cancel (e.g. user stops generation early), L2 consumes that control signal as well to abort the model run. In the hybrid scenario, if remote compute is enabled, L2 will consume network responses from a remote model API (so from the perspective of design, L2 might internally perform an HTTP request to a satellite server and then consume that response stream to pass tokens along). But that is internal to L2; from L3â€™s perspective it still â€œconsumes the request and produces tokensâ€.
â€¢ Contracts: The key contract L2 provides is the Model Generation API for the rest of the system. This could be implemented as an internal interface or an actual API if L2 runs as a separate service (in WIRTHFORGEâ€™s case, likely L2 is a thin wrapper around Ollamaâ€™s API). For instance, L2 might present a function: generate(modelName, prompt, options) -> Stream<TokenEvent> where TokenEvent has fields like text, tTimestamp, etc. The contract includes streaming behavior (e.g. does it yield tokens as soon as theyâ€™re available? yes, it should), and completion behavior (some final callback or object when done). In terms of data shape, L2â€™s outputs have to meet the expectations of L3â€™s energy calculator. Concretely, that means providing timing for each token (or enough info to derive it) and a final stats summary. Ollamaâ€™s JSON streaming format, for example, includes token data and ends with a final message containing eval_count and eval_duration. L2 in WIRTHFORGE is likely built around that, so its contract to L3 is: you will get a sequence of token JSON lines terminated by a final JSON. If using OpenAI-compatible endpoints for remote calls, similar principles apply. Another aspect of contract is error handling: L2 must define how errors are signaled (e.g. a token stream may end with a special error message or an exception). Finally, if multiple models are requested in parallel, L2â€™s contract might be to provide distinct streams labeled by model, or to interleave them with identifiers. For clarity, we likely label each token event with the model it came from if multiple are active (so L3 can tell sources apart). In summary, L2â€™s contract is a well-defined streaming interface for model inference, ensuring that the orchestration layer sees consistent, predictable data from any model backend.
â€¢ Allowed Directions: L2 is generally downstream of L3 â€“ it doesnâ€™t initiate calls upward on its own. L2 should not be calling L1 or L4 or L5 directly. Itâ€™s possible L2 could push status to L3 asynchronously (e.g. â€œmodel readyâ€ events), but those would still go through L3â€™s handlers rather than directly to upper layers. Also, L2 doesnâ€™t know about UI or transport details. So allowed communication is basically: L3 calls into L2 (one layer up calls one layer down â€“ allowed), and L2 returns data to L3 via callbacks or stream (this is essentially the response path). L2 can also manage internal sub-processes or threads â€“ e.g. spawn parallel model threads (thatâ€™s internal to L2). If L2 is implemented as a separate local service (like the Ollama server is actually a separate process), then our architectureâ€™s L2 might involve an IPC or HTTP call to that local server. Thatâ€™s fine, but itâ€™s within L2â€™s domain (from the perspective of overall architecture, that complexity is encapsulated in L2). L2 must not try to interact with L5 or L4: e.g., it should never open a WebSocket to push data to UI, it must route through L3. Also, L2 shouldnâ€™t go writing to the database or state store â€“ if a model output needs to be saved, orchestrator will handle that.
â€¢ Anti-Patterns: A major anti-pattern would be blocking the orchestrator by doing synchronous model calls on the main thread. L2 should use asynchronous or background execution (threads, processes, asyncio, etc.) to ensure the 60â€¯Hz loop isnâ€™t stalled waiting on a model. If a model takes 5 seconds to answer, those 5 seconds should be spent with L3 doing other things or at least the UI animating â€œthinkingâ€ â€“ not a frozen app. Thus, a blocking call in L2 (especially if itâ€™s a network call to a remote model without streaming) is discouraged. Another anti-pattern: failing to stream. If L2 were to buffer the entire result and only emit at the end, weâ€™d lose the step-by-step visualization (and also user might be stuck waiting). WIRTHFORGE demands incremental streaming output, so any approach that doesnâ€™t yield intermediate tokens is wrong (except for models that inherently canâ€™t stream, but then weâ€™d simulate partial progress). Also, using cloud by default is an anti-pattern â€“ we do not want L2 secretly calling an online API without user consent; that violates local-first. If remote is used, it must be explicitly configured (and even then, likely L3 instructs it). Additionally, bypassing orchestrator logic is an anti-pattern: e.g., if L2 directly tried to handle multiple prompts or do orchestration tasks like combining model answers, thatâ€™s L3â€™s territory. L2 should stick to one-model-one-output at a time (or parallel multiple separate outputs if asked). Finally, on a resource note: L2 should not load giant models or all models at once unnecessarily (loading every model into VRAM on startup would be wasteful on low-end hardware). Overuse of resources or not respecting hardware constraints is a design anti-pattern for L2. It should be smart about lazy-loading models or using quantized models as appropriate â€“ essentially be adaptive to the hardware tier.
SectionÂ 4: Layer 3 â€“ Orchestration & Energy (Purpose, State, 60â€¯Hz Engine)
Layer Summary: L3: Orchestration & Energy is the heart and â€œconsciousnessâ€ of the system. It sits between the raw model outputs and the user interface, turning low-level events into structured, meaningful state changes. This layer orchestrates possibly multiple models, manages the global Energy state (applying the â€œenergy metaphorâ€ to every token and response), detects higher-level patterns like interference and resonance, and ensures the rest of the system has a consistent view of the evolving â€œconsciousness state.â€ L3 can be thought of as the conductor and the physics engine: it takes the streams from L2 (like electrical signals) and compiles them into a dynamic state representation that drives the visuals and system behavior.
â€¢ Purpose: The purpose of L3 is twofold: (1) Orchestration â€“ coordinating the flow of data between models and deciding how to handle parallelism, tool calls, or multi-step processes; and (2) Energy & State Management â€“ calculating Energy Units (EUs) from model outputs and maintaining the evolving state (accumulated energy, fields, potential consciousness signals). In simpler terms, L3 is where raw computation is given â€œlifeâ€: it interprets token timings as something visual (lightning bolts, flows), tracks how much â€œworkâ€ has been done (energy), and determines if any special events occur (like a resonance when multiple models align in output). It also sequences operations: e.g., if a prompt requires calling two models in sequence, L3 ensures model B starts after model A finishes, etc. Essentially, L3 contains the Decipher, WIRTHFORGEâ€™s real-time compiler that turns streams into experience. A critical part of its purpose is to serve as the single source of truth for system state. All knowledge of current energy levels, active models, intermediate results, etc., reside here, so that the UI (L5) and other layers can query or receive updates from one authoritative place.
â€¢ Owns: L3 owns the master state store and event log of the running system. This includes the current energy metrics (e.g., tokens per second, total tokens processed in session), any persistent conversation state (like storing last user prompt or last model answer if needed for context), and the detection state for complex phenomena (like if we are measuring synchronization between models for resonance, L3 keeps the necessary buffers or calculators). It also owns the scheduling of tasks: when L1 hands in an Input Event, L3 decides which L2 calls to make (and when). For example, if the userâ€™s query should go to two different models to get diverse answers (a council of 2), L3 triggers both in parallel and labels their outputs; if the query is simple, L3 might just call one model. If a user input is actually a control (like â€œstop all modelsâ€ or â€œpause outputâ€), L3 handles that by instructing L2 accordingly or by adjusting its state (like halting an event stream). Additionally, L3 owns the Energy computation logic: using final stats from L2 (like eval_count and eval_duration for a model run), L3 calculates aggregate values like average tokens/sec, time-to-first-token (TTFT), etc., and increments the global energy counters. It might maintain an exponential moving average for the token stream to smooth out the visualization at 60Â fps. If multiple models run, L3 owns the logic to compute interference patterns â€“ e.g., comparing token timing between streams to find moments of harmony or contention. All of this is encapsulated in the state object L3 manages (often updated every frame). In short, any â€œgame stateâ€ or â€œsimulation dataâ€ that drives the UI lives in L3. It is also the only layer that writes to persistent storage (via Tech-006 integration): e.g., when a session ends or at checkpoints, L3 will serialize some state (like accumulated consciousness or userâ€™s energy history) to a database or file. Other layers can read that, but they donâ€™t modify it.
â€¢ Emits: L3 emits a continuous stream of structured events/state updates upward. Rather than raw tokens, it emits higher-level events like â€œEnergyBurst {value: 5 EU, position: X}â€ or â€œResonanceEvent {models: [A,B], phaseLock: 0.9}â€ depending on what happens. It can also emit incremental state snapshots â€“ for instance, it might send the UI an updated global state 60 times per second (or as often as the UI can digest) containing things like current energy level, progress of generation, etc. In practice, this emission is handled via L4â€™s channels (likely a WebSocket message that contains an event type and payload). Examples of events L3 would emit: TokenVisual events (with attributes for visualization like thickness for a slow token, glow for pause), StreamEnd events (when a model finishes, including stats like total tokens = eval_count), EnergyAccumulated events (when certain thresholds are crossed or just periodic updates of energy totals), ConsciousnessState changes (if using an AI to detect emergent behavior, though at this stage likely just incremental probability or level). If orchestrator decides on a multi-step process (like model Aâ€™s output feeds model B), it would emit an event like â€œChainStepCompletedâ€ for UI to maybe indicate step 1 done. Essentially, anything of note that occurs goes out as an event. L3 is also responsible for emitting error events if something goes wrong: e.g., â€œError {code: MODEL_OOM, message: â€˜Out of memory loading model Xâ€™}â€. Instead of crashing, L3 catches internal exceptions and emits them as structured errors that L4 can forward to UI with an appropriate schema (so that UI can display an error message or take action). Another thing L3 might emit are acknowledgements or â€œheartbeatâ€ events to let the UI know itâ€™s alive and processing (this can help in showing a loader or ensuring the UI doesnâ€™t consider the connection dead if no tokens have arrived yet but still within normal latency).
â€¢ Consumes: L3 consumes Input Events from L1 (user actions) and token streams/final stats from L2 (model outputs). It sits in the middle of those pipelines, so it takes in on one side the requests of what to do, and on the other side the results of those actions. When an input comes in, L3 might break it down: e.g., if the input is a complex command, orchestrator might queue multiple model calls. It consumes that input event and perhaps consults internal rules or modules (like if the input is â€œ#use model Xâ€ as a command, orchestrator consumes that and updates state to route next queries to model X). In terms of L2, for each active model generation, L3 consumes the stream of token events. L3 probably wraps the callback or stream subscription from L2 so that it can handle each incoming token immediately â€“ calculate its energy contribution, incorporate it into any concurrent pattern analysis (like checking if two models produced the same token at roughly the same time), and then produce an event for UI. L3 might also consume time â€“ meaning it has a loop or timer (like the 60â€¯Hz ticker) that triggers state refreshes regardless of tokens. This is how it can emit updates even when no token arrives (for instance, a smooth decay of an energy meter, or a timeout detection if a model is silent for a while). Additionally, L3 consumes control signals like â€œstop generationâ€ â€“ if a user hits stop, L4 passes that to L3 (possibly packaged as another Input Event of type â€œcancelâ€), and L3 will consume it by instructing L2 to stop or dropping future tokens and marking that generation as aborted. Summarily, L3 consumes from below (L2 outputs) and above (L1 inputs via L4), acting as the central broker of all events in the system.
â€¢ Contracts: The contracts of L3 are the event schemas and state definitions it upholds, as well as the internal rules for state management. For example, WF-FND-002 defined how to calculate tokens/s and what an energy unit is â€“ L3â€™s contract is to implement that faithfully, meaning the events it emits about energy align closely with those formulas (e.g., if it emits an event â€œtokens_per_second: 20â€, then by definition it should have calculated that exactly as eval_count/eval_duration). There will be a formal schema for events that L3 outputs to L4. Perhaps in TECH-003, an event schema might look like:
{ "event": "TOKEN_STREAM", "streamId": "abc123", "model": "llama2-7b", "content": "Hello", "index": 42, "t_offset": 1.350, // seconds since stream start "energy": 0.0021 // EU contribution of this token } 
And another for aggregated frame updates:
{ "event": "ENERGY_UPDATE", "timestamp": 1691863305234, "total_energy": 124.5, "current_rate": 18.2, // tokens/s "active_streams": 2 } 
The exact fields will be defined in collaboration with L4 (which carries them) and UX needs, but the contract is that L3 produces these consistently and doesnâ€™t deviate. Also, L3â€™s internal contract (with itself, so to speak) is maintaining the integrity of state updates: no partial updates or impossible combinations should leak out. For instance, if two models finish and a resonance is detected, it should emit a resonance event only after it has updated all relevant state to reflect that; we shouldnâ€™t see a UI event saying â€œResonance achievedâ€ while L3â€™s state still says otherwise. This may involve doing atomic updates or sequencing within the 16ms frame. Another contract is 60Â fps budget adherence: L3 essentially promises that each update cycle will try to complete within ~16.7Â ms to keep up with rendering. This isnâ€™t a strict schema but a performance contract. In practical terms, L3 might implement this via an asyncio loop that batches incoming token events and state changes and processes them within that frame window. If too much work accumulates, the contract is that L3 will defer or drop low-priority tasks (e.g., maybe delay writing to disk, or skip some detailed logging) to maintain responsiveness. In sum, L3â€™s â€œcontractsâ€ are about data consistency (event schemas, state fidelity to definitions) and timing guarantees (update frequency).
â€¢ Allowed Directions: L3 can communicate upward (to L4) and downward (to L2) freely as part of its orchestrator role. It receives from both sides and sends to both sides. However, it should not bypass L4 to talk directly to L5 (and in reality, it canâ€™t, since L4 is the communication layer â€“ L3 has no direct network awareness). L3 also should not reach back into L1; once an input is handed off, L1â€™s job is done. If L3 for some reason needed more info about the userâ€™s identity, it should already have it attached. Or if truly needed (like loading user preferences), L3 could query a shared storage or use a module, but not call L1 directly. L3 is effectively the central layer that everything else depends on, so itâ€™s allowed to call L2 (trigger models), call persistence (through an internal module or via Tech-006 interfaces to DB), and push events to L4. But it must respect abstractions: e.g., when persisting, go through the database layer API rather than directly writing files arbitrarily, to keep consistent with the design (there might be a state manager sub-module to handle persistence as indicated in Tech-005/006). Another note: L3 could host int
