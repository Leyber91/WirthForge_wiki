# WF-TECH-002: Local AI Integration & Turbo/Broker

## ðŸ§¬ Document DNA

**Unique ID**: WF-TECH-002
**Category**: TECH
**Priority**: P0
**Dev Phase**: 2 (Core Tech)
**Estimated Length**: ~4,500 words
**Document Type**: Technical Specification & Implementation Guide

## ðŸ”— Dependency Matrix

### Required Dependencies

* **WF-TECH-001 â€“ Automated Startup & Orchestration**: Provides orchestrator lifecycle, 60Hz loop, and transport endpoints that TECH-002 integrates with
* **WF-FND-001 â€“ Manifesto & Vision**: Enforces local-first, energy-truth, and web-engaged local-core principles
* **WF-FND-002 â€“ Energy Metaphor**: Defines token-level signal capture (Î”t, TPS, entropy) and E(t) âˆˆ [0,1] computation
* **WF-FND-004 â€“ DECIPHER**: Specifies token stream ingestion, backpressure handling, and event payload formats
* **WF-FND-005 â€“ Orchestration & Consciousness**: Provides tier policies, council sizing, and hybrid/broker governance
* **WF-FND-006 â€“ System Governance**: Enforces no-Docker constraints, schema versioning, and privacy preservation

### Enabled Dependencies

* **WF-TECH-003 â€“ Protocol Synthesis**: Builds on model pool and token streams for protocol message generation
* **WF-TECH-004 â€“ State Management**: Consumes energy signals E(t) and model outputs for persistence
* **WF-TECH-005 â€“ Web Interface**: Displays real-time model telemetry and energy visualizations
* **WF-TECH-006 â€“ Integration Testing**: Validates Ollama bindings and performance benchmarks require our council/turbo outputs.

* **Crossâ€‘References**

  * **WFâ€‘METAâ€‘004 â€“ TECH Prompts.** Confirms scope, deliverables, and â€œwebâ€‘engaged localâ€‘coreâ€ posture specific to TECHâ€‘002.

## ðŸŽ¯ Core Objective

Implement **native, zeroâ€‘docker** integration with **Ollama** as the firstâ€‘class local model runtime, including: streaming token capture, **tokenâ†’EU** telemetry for DECIPHER, **tierâ€‘aware model pool** management, and an optional **Turbo** ensemble (local parallel models) â€” all under the *webâ€‘engaged localâ€‘core* constraint (browser UI required; computation remains on device). Hybrid **Broker** use is *optâ€‘in*, never required. Success is measured by realâ€‘time streams that preserve **energyâ€‘truth** at **60â€¯Hz** with governance invariants intact.

## ðŸ“š Knowledge Integration Checklist

* **Energy math** (Î”t, TPS, topâ€‘K/entropy, DI, E(t) normalization & smoothing) implemented per WFâ€‘FNDâ€‘002; tolerate missing logprobs via graceful degradation.&#x20;
* **DECIPHER contracts**: nonâ€‘blocking token ingestion, 60â€¯Hz framing, drop/degrade/backpressure rules.&#x20;
* **Council/Tier policies**: cap parallel models by tier (Low/Mid/High/Hybrid) and progression level; unlock Turbo only where allowed.
* **Governance invariants**: localâ€‘core, **no Docker**, visible energy, UI presence; schema versioning & audit hooks.
* **Webâ€‘engaged localâ€‘core**: bind to local Web API and WebSockets; no core cloud dependency.&#x20;

## ðŸ“ Content Architecture

### Section 1: Opening Hook â€” â€œPlug the Engine Inâ€

TECHâ€‘002 is the **engine mount**: it binds the orchestrator (TECHâ€‘001) to a real, local model runtime (Ollama), streams tokens immediately, converts them to **E(t)**, and drives the UI at **60â€¯Hz** without any cloud calls. From tierâ€‘aware **singleâ€‘model** use at LevelÂ 1 to **Turbo** local councils at higher tiers, every emitted visual is grounded in measured signals â€” no fakery, just **energyâ€‘truth**.

### Section 2: Core Concepts

1. **Ollama Adapter (Layerâ€‘2 binding).**

   * Responsibilities: health/proc control (`serve` presence), **streaming** tokens (async), and telemetry capture (timestamps, Î”t, TPS, optional topâ€‘K/entropy).
   * Guarantees: localâ€‘only I/O; no Docker; backpressure & abort signaling; deterministic timestamps.&#x20;

2. **Model Pool & Load Policy.**

   * Maintain a **tierâ€‘aware** pool of loaded models with limits for `max_parallel_models`, `max_loaded_models`, VRAM/RAM budgeting, and prefetch rules. Defaults derive from FNDâ€‘005 tier caps and progression.&#x20;

3. **Turbo Ensemble (Local Council).**

   * Orchestrate **4â€“6** concurrent local models on High/Hybrid tiers; stream interleaved tokens; compute **DI** and ensemble **E\_ensemble(t)** per FNDâ€‘002; never block the 60â€¯Hz loop.

4. **Hybrid Broker (Optional, Userâ€‘Controlled).**

   * If **Hybrid** is configured, allow a *satellite assist* path without changing the localâ€‘core truth; never required for core flows; gated by policy (`broker_support` / plan).&#x20;

5. **Tokenâ†’EU Mapping & Smoothing.**

   * Compute E(t) from cadence, certainty, and stall; maintain EMA for smooth visuals; emit perâ€‘frame energy metrics at **60â€¯Hz** to DECIPHER.

### Section 3: Implementation Details

#### 3.1 Interfaces & Data Flow

**Launch Path (happy path):** Orchestratorâ†’`OllamaAdapter.ensure_running()`â†’`ModelPool.ensure_loaded(model)`â†’`OllamaAdapter.generate_stream(prompt, params)`â†’ perâ€‘token eventsâ†’**DECIPHER.ingest(token, meta)**â†’ perâ€‘frame **E(t)**â†’ WebSocket (**TECHâ€‘003**).

```python
# Pseudocode (asyncio/FastAPI compatible, no Docker)
class OllamaAdapter:
    def __init__(self, host="127.0.0.1", port=11434):
        self.base = f"http://{host}:{port}"

    async def ensure_running(self) -> None:
        if not await self._ping():
            # spawn local process; equivalent to `ollama serve` without docker
            await self._spawn_ollama_daemon_local()
        assert await self._ping(), "Ollama not reachable"

    async def generate_stream(self, model:str, prompt:str, **kw):
        # returns async iterator of {token, t_ms, delta_ms, topk?}
        async for chunk in self._http_stream("/api/generate", json={"model": model, "prompt": prompt, **kw, "stream": True}):
            yield chunk  # { "token": "...", "t_ms": 123, "logprobs": [...]? }

class ModelPool:
    def __init__(self, tier_policy):  # from FND-005 capabilities
        self.tier = tier_policy
        self.loaded = {}

    async def ensure_loaded(self, model:str):
        if model in self.loaded: return
        await self._pull_if_missing(model)  # local or bundled
        await self._load(model)             # warm-up; respects tier caps

class EUMapper:
    def compute(self, token_event) -> float:
        # E(t) in [0,1] from cadence + certainty + stall (FND-002)
        return self._normalize(token_event)

async def run_single(prompt:str, model:str, dec):
    await ollama.ensure_running()
    await pool.ensure_loaded(model)
    async for e in ollama.generate_stream(model, prompt):
        e["E"] = eu.compute(e)
        dec.ingest(e)  # feeds DECIPHER; 60 Hz frames emit downstream
```

*Notes:* token timestamps must be monotonic; **abort** (user stop) must cancel the HTTP stream and propagate an orchestrator stop event; **backpressure** uses DECIPHERâ€™s queue thresholds before requesting slowâ€‘down.&#x20;

#### 3.2 Turbo Ensemble (Local Council)

```python
async def run_turbo(prompt:str, models:list[str], dec):
    # Constrain by tier policy, e.g., <= 6 models high-tier
    models = models[:tier.max_parallel_models]
    async def one(m):
        async for e in ollama.generate_stream(m, prompt):
            e["source"] = m
            e["E"] = eu.compute(e)
            dec.ingest(e)
    await asyncio.gather(*(one(m) for m in models))  # interleave tokens
```

* **DI & Ensemble Energy:** compute DI on aligned token windows (if topâ€‘K available) and derive **E\_ensemble(t) = Î£ Î³\_m E\_m(t)** (clamped). Emit interference/resonance cues to UI via DECIPHER/TECHâ€‘003.&#x20;
* **Caps & Gating:** obey tier caps (e.g., Highâ‰ˆ6, Midâ‰ˆ4, Lowâ‰ˆ2), with Hybrid allowed but *off by default* unless user optsâ€‘in.&#x20;

#### 3.3 Model Pool & Policy

* **Caps:** `max_parallel_models`, `max_loaded_models`, `max_model_size`.
* **Budgeting:** guardrails for VRAM/RAM; **lazy load** + **warmâ€‘up** to avoid TTFT spikes.
* **Eviction:** LRU with coolâ€‘down to prevent thrash.
* **Config Sources:** tier YAML / capabilities JSON from FNDâ€‘005; versioned under governance.

#### 3.4 Webâ€‘Engaged Localâ€‘Core Endpoints (served by FastAPI)

* `GET /models` â†’ list local models + status.
* `POST /models/load {name}` â†’ ensure present, load/warm.
* `POST /generate {model, prompt, â€¦}` â†’ start stream (Serverâ€‘Sent / WS bridge).
* `POST /stop {session_id}` â†’ cancel stream.
* `GET /stats` â†’ perâ€‘model TPS, TTFT, EU rate (for Evidence Mode).
  All endpoints bind to **127.0.0.1**, with schema versions tracked per FNDâ€‘006.

#### 3.5 Backpressure, Abort, & Degrade

* **Backpressure:** apply DECIPHER queue watermarks â†’ batch/merge lowâ€‘impact tokens; request slowâ€‘down if supported.
* **Abort:** user stop closes streams and flushes frame composers cleanly.
* **Degrade:** preserve 60â€¯Hz by prioritizing tokenâ†’EU and basic frame emission; delay higherâ€‘order analytics if needed.&#x20;

#### 3.6 Antiâ€‘Patterns (Do Not)

* Buffer entire completion before streaming.
* Emit visuals not backed by measured signals (â€œno smoke & mirrorsâ€).&#x20;
* Exceed tier caps or bypass governance invariants (e.g., invoking Docker).&#x20;

### Section 4: Integration Points

* **TECHâ€‘001 (Orchestrator):** our adapter exposes `generate_stream`, `ensure_running`, and pool APIs; orchestrator schedules these in its event loop.
* **TECHâ€‘003 (Realâ€‘Time Protocol):** token/EU frames â†’ `energy.*`; council events â†’ `council.*`; lifecycle â†’ `experience.*`.&#x20;
* **WFâ€‘FNDâ€‘004 (DECIPHER):** we are the upstream source; we conform to its ingestion and 60â€¯Hz frame composition timing.&#x20;
* **WFâ€‘FNDâ€‘005 (Progression/Tiers):** Turbo only when allowed; hybrid broker optional and flagged; policies are dataâ€‘driven (JSON/YAML).
* **WFâ€‘FNDâ€‘006 (Governance):** release checks ensure **no Docker**, localâ€‘core, schema/version compliance.&#x20;

### Section 5: Validation & Metrics

* **Startupâ€‘toâ€‘Firstâ€‘Token (TTFT):** baseline & warmed TTFT by tier/model; median & p95 reported.
* **60â€¯Hz Integrity:** frame budget adherence under single/ensemble loads; **â‰¥55â€¯Hz** on low tier; **60â€¯Hz target** generally.&#x20;
* **Energyâ€‘Truth Consistency:** crossâ€‘check `E(t)` against raw signals; Evidence Mode exposes WVMP metrics.&#x20;
* **Tier Caps Compliance:** never exceed `max_parallel_models` and resource budgets.&#x20;
* **Localâ€‘Only Traffic:** audit shows no external calls in core flow; hybrid only when toggled.&#x20;
* **Schema Versioning:** APIs/events carry semver; contract tests gate changes.&#x20;

## ðŸŽ¨ Technical Deliverables

All deliverables are implemented as separate files in the `TECH-002/` directory for modularity and maintainability, following the established TECH-001 pattern.

### Architecture & Design Documentation

* **[WF-TECH-002-OLLAMA-ADAPTER.md](TECH-002/WF-TECH-002-OLLAMA-ADAPTER.md)** â€” Native Ollama integration architecture, process management, token streaming, and energy telemetry capture
* **[WF-TECH-002-TURBO-ENSEMBLE.md](TECH-002/WF-TECH-002-TURBO-ENSEMBLE.md)** â€” Multi-model council orchestration (4-6 models), ensemble energy computation, and diversity index calculation
* **[WF-TECH-002-HYBRID-BROKER.md](TECH-002/WF-TECH-002-HYBRID-BROKER.md)** â€” Optional satellite assistance with user-controlled privacy and local-core primacy

### Configuration & Policies

* **[WF-TECH-002-TIER-POLICY.yaml](TECH-002/WF-TECH-002-TIER-POLICY.yaml)** â€” Tier-based resource allocation, model pool management, and performance thresholds
* **[WF-TECH-002-API-SCHEMAS.json](TECH-002/WF-TECH-002-API-SCHEMAS.json)** â€” Complete API contract definitions with request/response schemas and event specifications

### Implementation Components

* **[WF-TECH-002-ENERGY-MAPPING.py](TECH-002/WF-TECH-002-ENERGY-MAPPING.py)** â€” Energy computation E(t) from token events, EMA smoothing, and diversity index calculation
* **[WF-TECH-002-FASTAPI-ENDPOINTS.py](TECH-002/WF-TECH-002-FASTAPI-ENDPOINTS.py)** â€” Local web API endpoints for model control, streaming, and real-time statistics (127.0.0.1 binding)

### Quality Assurance

* **[WF-TECH-002-INTEGRATION-TESTS.py](TECH-002/WF-TECH-002-INTEGRATION-TESTS.py)** â€” Comprehensive test suite for 60Hz compliance, schema validation, and tier policy enforcement
* **[WF-TECH-002-BENCHMARKS.md](TECH-002/WF-TECH-002-BENCHMARKS.md)** â€” Performance benchmarks with TTFT/TPS targets, memory usage analysis, and regression testing criteria

## âœ… Quality Validation Criteria

* **Architecture correctness:** adapter â†” DECIPHER â†” transport boundaries respected; no crossâ€‘layer leaks.&#x20;
* **Governance invariants:** **localâ€‘core**, **no Docker**, **60â€¯fps**, energyâ€‘truth, UI presence â€” all verifiably true in code/tests.&#x20;
* **Performance:** sustained 60â€¯Hz under singleâ€‘model load; tierâ€‘appropriate throughput for Turbo with graceful degrade.&#x20;
* **Schema/versioning:** semver headers, contract tests, auditability per FNDâ€‘006.&#x20;
* **Clarity & testability:** each deliverable runnable/linteable; endpoints & data types unambiguous (examples included).&#x20;

## ðŸ”„ Post-Generation Protocol

* **Glossary update:** add/confirm entries for *Ollama Adapter*, *Turbo Ensemble*, *DI*, *E(t) smoothing*, *TTFT*. (Master glossary under FNDâ€‘006.)&#x20;
* **Dependency graph:** link TECHâ€‘002 under TECHâ€‘001 and as producer to TECHâ€‘003/005; update doc index.&#x20;
* **Versioning:** tag initial **v1.0.0** for TECHâ€‘002; create `CHANGELOG-WF-TECH-002.md`; schemas carry `xâ€‘schemaâ€‘version`.&#x20;
* **Assets & repo:** commit deliverables to `/docs/tech-002/` and `/code/tech-002/`; register in assets manifest and style map (consistent with TECHâ€‘001 split).&#x20;
* **Cascade queue:**

  * **TECHâ€‘003:** finalize `energy.*` / `council.*` channel messages using our event shapes.&#x20;
  * **TECHâ€‘005:** bind frame composer to our EU mapper & DI utilities.&#x20;
  * **UXâ€‘001/002:** wire Evidence Mode overlays to `/stats` & perâ€‘frame energy metrics.&#x20;

---

**Notes aligned to your last revision:** This document is authored to **match the TECHâ€‘001 deliverable split pattern** you adopted (separate files, main doc references only). It generalizes across all hardware tiers, assumes **Python + asyncio** with **FastAPI** for transport, and maintains **automatic Ollama setup** so users need not configure anything manually â€” fully consistent with the *webâ€‘engaged localâ€‘core* architecture and governance invariants.
